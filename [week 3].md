# 3주차_강화학습

태그: 완료
주차 내용: class with pytorch,환경비교

## 강화학습

- 가치함수를 계산이 아닌 경험을 통해 찾는다. 이때 경험은 에이전트와 환경의 상호작용으로 발생한 상태, 행동, 보상을 일컫는다. 강화학습은 모델 없이도 학습 가능하며 상태-행동이 매칭되어 있는 행동가치가 유용하게 쓰인다.
- 경험의 구조
    1. 어떤 행동을 시도한다.
    2. 보상에 따라 행동의 가치를 평가한다.
    3. 평가한 값을 기준으로 정책을 업데이트한다.
    4. 1-3을 반복한다. 
- 정책 이터레이션(GPI)-강화학습, 정책 평가-예측, 정책 발전-제어와 대응한다.

## 몬테 카를로

- 가치 함수를 계산하는 것이 아니라 상태-행동에 대한 이득의 표본을 모으고 평균을 내어 가치 함수를 학습(예측)한다는 점에서 강화 학습과 맞닿아 있다. 표본의 수가 증가하면 그 평균값이 기댓값과 같아진다는 큰 수의 법칙을 따른다.
- 한 번의 에피소드가 끝날 때 모든 상태에 대한 가치 함수가 업데이트 된다.
- 몬테 카를로 예측:  $V_{n+1}=V_{n}+\frac{1}{n}(G_{n}-V_{n})$의 식으로 가치함수를 업데이트하며, 밑첨자 n은 n번째 에피소드임을 나타낸다. 궁극적으로 도달하고자 하는 목표는 반환값 $G_{n}$로, 에피소드의 총보상이기 때문에 한 번의 에피소드가 끝난 이후에야 가치 함수의 업데이트가 가능하다. $\frac{1}{n}$은 step size로, 상수 $\alpha$로 표현한다. 이때 상수 $\alpha$를 사용해 값을 고정하는 것은 과거의 반환값보다 현재 에피소드의 반환값을 더 중요하게 생각함을 의미한다. 만약 $\alpha$의 값이 커진다면 이는 과거의 반환값의 중요도를 지수적으로 감소시킨다는 것을 의미한다.
    - 예측 방법: 에피소드를 진행할 때 상태 s를 여러 번 마주칠 수 있는데, 이를 s와의 최초 접촉이라고 한다. 최초 접촉 MC 방법은 최초 접촉 이후 발생하는 이득의 평균을 구하는 방법으로 가장 통상적으로 쓰인다.
- 몬테 카를로 제어: 행동 가치 함수를 이용하기 때문에 모델 없이도 탐욕적으로 정책을 구할 수 있다.

## 시간차 학습

- 시간차 학습은 몬테 카를로의 장점인 ‘모델 없이 표본 이득을 통해 가치 함수를 찾을 수 있다는 점’과 DP의 장점인 ‘시간 단계 별로 가치 함수를 업데이트 할 수 있다’는 점을 합친 방법론이다.
- 시간차 예측(TD): 벨만 기대 방정식에서 출발한다. DP에서는 벨만 기대 방정식의 해를 찾으려고 계산한 거였지만 TD에서는 $R_{t+1}+\gamma V_{\pi}(S_{t+1})$의 값을 **샘플링**해 가치 함수를 구한다.  $V(S_{t})←V(S_{})+\alpha (R_{t+1}+\gamma V(S_{t+1})-V(S_{t}))$로 가치 함수를 계산하기 때문에 시간 스텝 별로 바로바로 가치 함수를 업데이트할 수 있다.
- on-Policy와 off-Policy: on-Policy는 행동 정책과 학습 정책이 같다. off-Policy는 행동 정책과 목표 정책을 분리한다. 행동정책은 자료를 생성하는 데 사용하고 목표 정책은 가장 좋은 정책을 학습하는 데 사용된다.
- 시간차 제어(SARSA): on-Policy다. $Q(s,a)←Q(s,a)+\alpha (R_{t+1}+\gamma Q(s',a')-Q(s,a))$처럼 행동 가치 함수를 구하고 이를 기반으로 탐욕적으로 행동을 선택한다. 현재 상태(s), 행동(a), 보상(r), 다음 상태(s’), 다음 상태의 행동(a’)이 필요하다.
- 부트스트랩: 예측값을 이용해 또다른 예측값을 찾는 방법이다. SARSA의 경우, 예측값 Q(s,a)를 구하기(업데이트하기) 위해 예측값 Q(s’,a’)를 이용한다.
- 탐험:  $\varepsilon$-탐욕 정책은 탐험, 즉 엉뚱한 행동을 하게 하는 정책이다. $\varepsilon$%의 확률로 랜덤한 행동을 하고 (100-$\varepsilon$)%의 확률로 탐욕적인 정책을 따른다.
- SARSA의 한계: 다음 행동의 가치가 현재의 행동의 가치에 영향을 끼친다는 점이다. 만약 다음 행동의 가치는 낮고 현재의 행동의 가치가 높다는 상반적인 가치를 갖고 있다면, 잘못된 학습이 진행될 수 있다.

## Q-러닝

- off-Policy이다. 행동 정책과 학습 정책을 분리하여 SARSA의 한계를 극복했다. $Q(s,a)←Q(s,a)+\alpha (R_{t+1}+\gamma max_{a}Q(s',a)-Q(s,a))$로 행동 가치 함수를 업데이트한다. 즉 현재 상태(s)에서 행동(a)를 해서 넘어간 다음 상태(s’)에서 행동할 때, SARSA처럼 $\varepsilon$-탐욕 정책을 따르는 것이 아닌 상태 s’에서 가장 큰 큐함수를 선택한다. 따라서 s,a,r,s’만으로도 학습할 수 있다.
- SARSA는 벨만 기대 방정식을, Q-러닝은 벨만 최적 방정식을 이용한다.



## 문제

1. 강화학습과 GPI의 구성을 설명하시오. 
    
    -GPI는 정책 평가와 정책 발전으로 이루어져 있다. 강화학습은 예측과 제어로 이루어져 있다. 정책 평가는 예측과 대응하며, 주어진 정책에 대한 가치 함수를 학습한다. 정책 발전은 제어와 대응하며, 가치 함수를 기반으로 정책을 발전시킨다. 
    
2. 몬테 카를로 방법은 (상태-행동) 쌍에 대한 보상을 반환한다.
    
    -o/ 몬테 카를로는 **모델이 없는 상황**에서 동작하고, 이때 (상태-행동) 쌍으로 이루어져 있는 **행동 가치 함수**가 유리하다. 따라서 보상도 이에 맞춰 이루어진다. 
    
3. 몬테 카를로 기법에서 발생하는 편향과 오류는 무엇인가?
    
    -길이가 길거나 무한대인 에피소드에 적합하지 않다. 몬테 카를로에서 가치 함수의 업데이트는 에피소드와 에피소드 사이에서만 일어나기 때문이다. 
    
4. 몬테 카를로 기법에서 샘플을 쌓는 방법은 무엇인가?
    
    -몬테 카를로에서 하나의 샘플이란 한 번의 에피소드를 의미한다. 한 번의 에피소드에서는 현 정책에 따라 거친 모든 상태에 대한 보상인 반환값을 목표로 가치 함수를 업데이트해 나간다. 따라서 많은 에피소드를 거쳐 평균을 내면 참 가치 함수를 구할 수 있다. 
    
5. 몬테 카를로, SARSA, Q러닝을 비교해 설명하시오.
    1. 몬테 카를로-SARSA
        
        -몬테 카를로는 에피소드의 반환값 $G_{t}$를 이용하기 때문에 한 번의 에피소드가 끝나야지 가치 함수의 업데이트가 일어난다. 하지만 SARSA는 $R_{t+1}+\gamma V(S_{t+1})$를 이용하기 때문에 실시간으로(타임 스텝 별로) 가치 함수를 업데이트할 수 있다. 
        
    2. SARSA-Q러닝
        
        -SARSA는 on-Policy로, 행동 정책과 목표 정책이 분리되어 있지 않아 매번 $\varepsilon$-탐욕 정책을 따른다. Q러닝은 off-Policy로, 행동 정책과 목표 정책이 분리되어 있어 다음 상태에서 행동을 선택할 때에는 무조건 탐욕적으로 행동한다. 이런 점에서 Q러닝이 SARSA의 한계를 극복했다고 할 수 있다. 또한 SARSA는 벨만 기대 방정식을, Q러닝은 벨만 최적 방정식을 이용한다는 점에서 각각 정책 이터레이션과 가치 이터레이션의 연장선이라고 볼 수 있다. 
        
    3. 업데이트에 필요한 것은 각각 무엇인가?
        1. 몬테 카를로는 특정 에피소드의 반환값 $G_{t}$, 특정 상태의 가치 함수 $V_{t}$가 필요하다. 
        2. SARSA는 현재 상태 S, 행동 A, 보상 R, 다음 상태 S’, 다음 행동 A’이 필요하다. 
        3. Q러닝은 현재 상태 S, 행동 A, 보상 R, 다음 상태 S’이 필요하다. 이때 다음 행동은 100%의 확률로 탐욕적으로 택하기 때문에 주어질 필요가 없다. 
6. 환경이 지속적으로 변화할 때 스텝사이즈를 평균으로 하는 것보다 상수로 고정하는 것이 좋은 이유?
    
    -과거의 반환값보다 현재의 반환값에 집중하는 것이 중요하기 때문이다. 이 외에도 수렴 속도가 빨라지고 수렴 안정성이 높아지며 수렴 복잡도가 줄어든다는 이점이 있다. 
    
7. $\varepsilon$-탐욕정책을 일반 탐욕정책 기준으로 설명하시오.
    
    -기존의 탐욕정책은 무조건 가장 높은 큐함수를 지닌 행동만을 취했다. 그러나 $\varepsilon$-탐욕정책은 $\varepsilon$%의 확률로 엉뚱한 행동을 하고 (100-$\varepsilon$)%의 확률로 탐욕적으로 행동한다는 점에서 기존의 탐욕 정책과 차이가 있다. 
    



## 질문

- 딥살사에서 인공신경망을 이용해 큐함수를 업데이트할 때 경사하강법과 MSE를 사용하여 오차함수를 정의한다. 이때 정답 레이블과 예측은 각각 무엇인가?
    
    $$
    정답: R_{t+1}+\gamma Q(S_{t+1}, A_{t+1}), 예측: Q(S_{t}, A_{t})
    $$
    
- 딥살사와 정책 기반 강화학습의 인공신경망의 입력과 출력은 각각 무엇인가?
    
    딥살사의 입력은 상태의 특징이고 출력은 행동에 대한 큐함수다. 정책 기반 강화학습의 입력은 상태의 특징이고 출력은 어떤 행동을 할 확률(정책)이다. 딥살사에는 큐함수를 근사하는 반면 정책 기반 강화학습에서는 정책을 근사하기 때문이다. 
    
- 딥살사와 정책 기반 강화학습에서 은닉층과 출력층에 쓰이는 활성함수는 각각 무엇인가?
    
    딥살사의 은닉층에서는 ReLU, 출력층에서는 선형함수가 쓰인다. 이때 큐함수의 범위가 0~1사이가 아니기 때문에 시그모이드가 아닌 선형함수가 쓰인다. 정책 기반 강화학습의 은닉층에서는 ReLU, 출력층에서는 softmax가 쓰인다. 여러 개의 행동에 대한 확률을 구해야 하기 때문이다. 
    
- 살사/큐러닝과 다르게 딥살사에서 $\varepsilon$-탐욕 정책에서 사용되는 $\varepsilon$의 크기가 줄어드는 이유는 무엇인가?
    
    초반에는 에이전트가 탐험을 통해 다양한 상황에 대해 학습하고, 학습이 충분히 이루어진 후에는 예측한 대로 움직이기 위해서이다. 
    
- 정책신경망을 업데이트할 때 오류함수를 최소화해야 하므로 경사 하강법을 이용한다.(o/x)
    
    X/ 딥살사에서는 오류함수를 최소화해야 하므로 경사 하강법을 사용하지만, 정책신경망에서는 목표함수를 최대화해야 하므로 경사 상승법을 사용해야 한다. 
    
- REINFORCE 알고리즘의 업데이트 기준은 무엇인가?
    
    반환값이다. 따라서 정책신경망의 업데이트는 한 에피소드가 끝난 이후에 실행된다. 
    



Class_with_Pytorch_%EB%B0%9C%ED%91%9C%EC%9E%90%EB%A3%8C.pdf](https://github.com/user-attachments/files/16655456/Class_with_Pytorch_.EB.B0.9C.ED.91.9C.EC.9E.90.EB.A3.8C.pdf)

[DP환경vs살사큐러닝환경.pdf][DP%ED%99%98%EA%B2%BDvs%EC%82%B4%EC%82%AC%ED%81%90%EB%9F%AC%EB%8B%9D%ED%99%98%EA%B2%BD.pdf](https://github.com/user-attachments/files/16655458/DP.ED.99.98.EA.B2.BDvs.EC.82.B4.EC.82.AC.ED.81.90.EB.9F.AC.EB.8B.9D.ED.99.98.EA.B2.BD.pdf)
