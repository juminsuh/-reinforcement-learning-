# 1주차_강화학습

태그: 완료

# 1. 머신러닝 개론

## 01. 인공지능


- 인간이 수행하는 지능적 작업을 자동화하는 도구

## 02. 머신러닝

- 기계가 테스크를 수행하는 방법을 스스로 학습하기를 바람
- 앨런 튜링: 메모리와 컴퓨터의 개념을 만듦+머신러닝 이론을 처음으로 고안

|  | 전통적 프로그램 | 머신러닝 |
| --- | --- | --- |
| input | 데이터+규칙 | 데이터+해답 |
| output | 해답 | 규칙 |

## 03. 머신러닝의 하위분야


### 지도학습과 비지도학습

- 지도학습(supervised learning)
    
    -학습 과정에서 답이 주어짐
    
    -답(y)와 예측값(y^) 사이의 오차를 최소화
    
    -ex. 회귀분석, 딥러닝
    
- 비지도학습(unsupervised learning)
    
    -학습 과정에서 답이 주어지지 x
    
    -주어진 데이터셋(x) 사이의 특정한 패턴/규칙을 찾아냄
    
    -ex. 군집화(clustering), 차원 축소(dimensionality reduction) 
    
- 강화학습은 지도학습? 비지도 학습?
    
    -강화학습에는 간접적인 해답인 “보상”이 존재하지만 직접적 정답은 x→지도학습 x
    
    -주어진 데이터로만 학습하지도 x→비지도학습 x
    
    ➡️ 명확히 분류되지 x
    

<aside>
💡 강화학습의 장점

정답을 명확히 라벨링하기 어려운 상황에서 보상을 통해 규칙을 찾을 수 있음

</aside>

# 02. 강화학습 개론

## 01. 강화학습


- 사전에 상황/행동에 대한 정보가 주어지지 x
- 그저 랜덤하게 행동을 수행하며 상/벌을 받고, 그 값을 매기고 갱신하여 결국 최대의 보상을 받을 수 있는 행동 집합을 구하고자 함

![Untitled](https://github.com/user-attachments/assets/6b1c841d-539e-4680-9809-4df7f9a9185e)

![Untitled 1](https://github.com/user-attachments/assets/6dc8ffbd-0c30-480c-a870-566434070efd)

$St$에 대한 $A_t$을 취하고, 다음 상태에서 $A_t$에 대한  $R_{t+1}$과  $S_{t+1}$이 주어짐. 일반적으로 보상은 행동의 다음 단계에서 주어짐(행동과 그 행동으로 인한 상태를 고려하여 주어지기 때문)

### 강화학습의 구성 요소(에이전트/환경)

- state_상태: 환경이 에이전트에게 제공하는 현재 상황에 대한 정보. 이때 에이전트는 사람처럼 스스로 문제에 대한 정의를 내리지 못하기 때문에, 문제 설정을 할 때 에이전트가 학습하기에 너무 많지도/적지도 않은 적절한 정보를 재공하는 것이 매우 중요함
- action_행동: 에이전트가 수행하는 대상. 보통 모든 상태에서 할 수 있는 행동은 동일함 (ex. 그리드월드에서, 모든 상태에서 상/하/좌/우로 행동할 수 있음)
- policy_정책: 에이전트의 행동 집합. 모든 상태에 대한 모든 행동의 확률을 적어둔 표
- reward_보상 신호: 에이전트의 행동에 대해 환경에 즉각 제공하는 숫자값. 이를 통해 행동이 좋은지 나쁜지를 간접적으로 확인 가능
- value_가치 함수: 보상의 총량. 이를 최대로 만드는 것이 강화학습의 목표. 따라서 에이전트는 보상보다 가치를 최대화하는 것에 주목함
- model_주변 환경에 대한 모델: 환경의 변화를 모사해 환경이 어떻게 변할 지를 추정할 수 있게 해줌. 즉 현재 상태/행동을 기반으로 다음 상태/보상을 예측함. 모델의 유무에 따라 모델 기반 방법과 모델 프리 방법으로 분류

![Untitled 2](https://github.com/user-attachments/assets/c83a8f17-a2b5-43a2-8990-22c1b5f40a95)

## 02. 강화학습을 수학적으로 정의하자!


### **순차적 행동 결정 문제**

순차적으로 선택을 하고, 그 선택을 바탕으로 또 다른 선택을 하는 과정

강화학습은 순차적 행동 결정 문제를 해결하는 하나의 방법

### **강화 학습 용어 수학적 재정의**


1. **에이전트와 환경은 연속되는 이산적 time step마다 상호작용**: 한 번의 강화를 위해서는 두 번의 time step이 필요. 이때 time step은 (t=0,1,2,3…)으로 이산적임. time step $t$에서 에이전트는 환경의 상태($S_t$)를 받고, 행동($A_t$)를 수행하며, 다음 time step $t+1$에서 환경은 에이전트에게 보상($R_{t+1}$)를 주고 새로운 상태($S_{t+1}$)을 제공함-확률 변수란 확률을 갖고 있는 변수를 의미. 이때 확률 변수는 대문자로 표현
2. **State($S_t$)**: 집합($S$)는 에이전트가 관찰 가능한 상태의 집합. 이때 시점  $t$에서의 상태는 고정되어 있지 않고 확률적으로 변하기 때문에, $S_t \in S$로 표현할 수 있음
3. **Action($A_t$)**: state($S_t$)에서 할 수 있는 행동의 집합. $A_t \in A(S)$ , 즉 특정 시점 $t$에서의 행동의 집합은 상태 $S$에서 할 수 있는 행동의 원소. 마찬가지로 확률 변수임
4. **Reward($R_{t+1}$)**:  $t$ 시점에서의    $S_t$와 $A_t$에 의존적인 이산 확률 분포를 가짐. state-action-reward에서 reward만 time step이 다른데, 이는 reward가 환경이 제공하는 것이며 에이전트의 action을 기준으로 하기 때문

### **MDP(Markov Decision Process)**

순차적 행동 결정 문제를 **수학적**으로 표현해 이론적으로 정교하게 설명하는 것. 이 과정은 **이산적인 시간에서의 확률 제어**. 부분적으로 랜덤성을, 부분적으로 제어성을 갖는 의사 결정 과정을 수학적으로 정의함

- **MDP의 과정**
    
    $S_t$에서 가능한 행동(A(S)) 중에 하나($A_t=a$)를 취함. 핵심은 $S_t(=s)$에서 $S_{t+1}(=s')$으로 전이될 때, **전이되는 방식이 확률적**이라는 것과 그에 따른 **보상($R_{t+1}=r$)**이 존재한다는 것
    
    ![Untitled 3](https://github.com/user-attachments/assets/27a9b7ca-f4a6-49bd-8597-c2797d3ae542)
    
**MDP의 dynamic을 정의하는 함수 p**

$$
p(s', r \mid s, a) = \text{Perc}(S_{t+1} = s', R_{t+1} = r \mid S_t = s, A_t = a)
$$
    
    
함수  $p$를 통해 환경에 대한 모든 정보를 계산할 수 있음


1. **상태 전이 함수**
    
    상태 $s$와 행동 $a$ 이후에 따라오는 상태 $s'$에 갈 수 있는 확률을 나타냄. 즉 **상태 s에서 행동 a를 한 이후에 상태 s’로 전이될 때 성공할 확률**이라고 해석되며, **MDP의 핵심.** 상태 변환 확률이라고도 하며 에이전트가 아닌 환경의 일부로서, 환경의 모델임
    
    
    $P_{ss'}^a=P(s'|s,a)=P[S_{t+1}=s'|S_t=s,A_t=a]=\sum_{r\in R}P(s',r|s,a)$
    
    
2. **보상 함수** 
    
    보상 함수가 기댓값($E)$로 표현되는 이유는 환경에 따라 같은 상태에서 같은 행동을 하더라도 다른 보상이 나올 수 있기 때문(* 기댓값이란 일종의 평균. 정확한 값이 아니라 나오게 될 숫자에 대한 예상)
    
    1. $r(s,a)=E[R_{t+1}|S_t=s, A_t=a]=\sum_{r\in R}r\sum_{s'\in S}P(s',r|s,a)$
    
    ➡️ $s$에서 $a$를 취할 때 주어지는 즉각적인 보상
    
    b.  $r(s,a,s')=E[R_{t+1}|S_t=s,A_t=a, S_{t+1}=s']=\sum_{r\in R}r*(s',r|s,a)/P(s'|s,a)$
    
    ➡️ $s$에서  $a$를 취해 $s'$으로 상태가 전이될 때 주어지는 보상
        

### **강화학습의 학습 목표와 보상**

보상의 합을 최대화시키는 것. 즉 단기적인 보상이 아닌 장기적인 총 보상을 목표로 해야 함. 따라서 강화학습은 보상이 아니라 **가치**를 우선시 함

### **강화학습의 학습 테스크와 가치**

강화학습은 에피소드 형식으로 학습됨. 에피소드는 유한한 시간 동안 에이전트와 환경이 상호 작용하는 것을 의미하며, 에피소드의 끝을 나타내는 종단 상태가 존재. 이때 총보상을 뜻하는 반환값  $G_t$는 종단 상태에 다다르는 시점까지의 보상의 총합

$$
G_t=R_{t+1}+R_{t+2}+...+R_T
$$

그러나 문제 발생!

1. 만약 명확한 종단 시점이 없는 연속적 상황일 때 문제가 발생. 시간이 무한히 길어지면 반환값도 무한히 발산하기 때문
2. 에이전트는 지금 받는 보상과 미래에 받는 보상을 구분하지 못함
3.  에이전트는 같은 보상을 받되, 그것이 한 번에 주어지는 것인지 여러 번에 나눠서 주어지는 것인지를 구분하지 못함

→이런 문제를 해결하고자 **할인**이라는 개념을 도입

### **할인**

할인이란 미래보상의 현재가치로 보상을 계산하는 것. 이때 미래의 보상을 현재로 땡겨서 미리 받으면 그 가치가 더 적어지고, 더 미래로 갈수록 그 정도가 심해진다는 할인율 개념이 도입됨. 할인은 각 타임 스텝의 보상에 할인률 $\gamma$(gamma)를 곱하는 방식으로 적용됨

$$
G_t=R_{t+1}+\gamma R_{t+2}+...=\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}
$$

$R_{t+k+1}$은 시점 $t$로부터 $k$만큼이 지났을 때의 보상을 의미하며 +1은 계산식 맞추는 용도

이때, 할인율은 [0,1] 사이의 값이기 때문에 종단 시점이 무한대로 발산하더라도 반환값($G_t$)은 특정 값으로 수렴함→문제 해결!

또한 할인율이 0에 가까울수록 근시안적으로 반환값을 다루고 1에 가까울수록 미래의 보상을 더 많이 고려하는 것으로 해석할 수 있어 표현력을 높여주는 효과가 있음

**MDP를 정의하기 위해 필요한 것: $(S, A, P, R, \gamma)$**

### **정책과 가치함수**

- **가치함수**
    
    가치함수란 현재 에이전트가 주어진 상태에 있는 게 얼마나 좋은가를 추정하는 함수. 즉 앞으로 받을 것이라 예상하는 보상의 기댓값이자 앞으로 나아가야 하는 상태에 대해 알 수 있음. 가치함수의 input은 state, ouput은 실수
    
    얼마나 좋은가는 기대되는 미래의 보상을 기준으로 하며, 이는 이전 행동에 따라 다름. 이전 행동은 정책에 의해 결정됨→정책과 가치함수는 유기적 관계
    

$$
v: State\to R
$$

- **정책($\pi)$**
    
    정책에는 모든 상태에서 에이전트가 수행할 행동이 정의되어 있음. 정책은 특정 상태에서 특정 행동을 할 확률. 이때 정책에서 어떤 행동을 할 지 결정하는 데에 가치함수가 이용됨
    
    정책에서 특정 상태에서 어떤 행동을 할 지는 **탐욕적(greedy)**으로 정해짐. 이는 선택지 중 가장 큰 값을 선택하는 방식. 따라서 정책에 있어 탐욕적인 선택은 모든 상태마다 가장 가치가 높은 행동을 선택함. →통상적으로 greedy algorithm은 항상 최적의 결과를 예측하지 않지만, 강화학습에선 모델이 학습할 때 미래의 것을 할인율의 개념으로 계산에 반영하고 학습에 사용되기 때문에 욕심쟁이 기법을 사용하면 결국 최적의 미래로 수렴하게 되는 것
    
    그러나 우리는 **최적 정책**을 알고 싶은 것. 최적 정책은 특정 상태에서 단 하나의 행동(최적 행동)만을 함
    
$$
\pi(a|s)=P(A_t=a|S_t=s)
$$
    

![Untitled 4](https://github.com/user-attachments/assets/dc932e65-a4cb-4383-bed5-c8f21af44afa)

1. 정책($\pi(a|s)$)을 통해 현재 상태($s$)에서 행동($a$)을 선택
2. 상태 전이 함수($p(s’|s,a$))를 통해 현재 상태에서 다음 상태로 확률적으로 전이됨
3. 수행한 행동과 다음 상태를 바탕으로 보상(r(s,a,s’))이 에이전트에게 주어짐

➡️ 에이전트는 학습하며 환경으로부터 보상을 받게 되고, 이것이 예상했던 보상인 가치 함수와 다르다는 것을 알게 됨. 이러한 과정에서 에이전트는 실제 받은 보상을 토대로 가치함수와 정책을 업데이트해 나감. 이 과정을 계속 반복한다면 가장 많이 보상을 얻게 하는 정책을 학습할 수 있음

### **정책으로 표현된 가치함수**

가치함수는 상태만을 기준으로 하는 **상태 가치 함수**와 행동을 기준으로 하는 **행동 가치 함수**로 분류


1. **정책 $\pi$에 대한 상태 가치 함수**
    
    상태  $s$에서 정책  $\pi$를 따랐을 경우 얻게 되는 이득(반환값)의 기댓값
    
$$
v_{\pi}(s)=E_{\pi}[G_t|S_t=s]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t_k+1}|S_t=s]=E_{\pi}[R_{t+1}+\gamma(R_{t+2}+\gamma R_{t+3}+...)|S_t=s]=E_{\pi}[R_{t+1}+\gamma G_{t+1}|S_t=t]=E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t=s]
$$

    
이때 두 번째 줄의 $G_{t+1}$은 반환값으로 표현했지만, 반환값은 실제로 에이전트가 받을 수 있으리라 기대하는 값이기 때문에 보상에 대한 기댓값인 가치 함수($v(s)$)로 나타낼 수 있음($\because v_{\pi}(s)=E_{\pi}[G_t]$)
    
$$
v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)
$$

    
1. $s$에서 $a$를 실행하는 것의 가치인 큐함수 $q_{\pi}(s,a)$와 $s$에서 $a$를 실행할 확률  $\pi(a|s)$를 곱함
2. 모든 행동에 대해 큐함수와 정책을 곱한 값을 더하면 $s$의 가치인 상태 가치 함수가 됨
2. **정책 $\pi$에 대한 행동 가치 함수(Q 함수)**
    
상태  $s$에서 행동 $a$를 취하고 그 이후로 정책 $\pi$를 따랐을 경우 얻게 되는 이득의 기댓값
    

$$
q_{\pi}(s,a)=E_{\pi}[G_t|S_t=s,A_t=a]=E_{\pi}[\sum_{k=0}^{\infty}\gamma^{k}R_{t_k+1}|S_t=s, A_t=a]=E_{\pi}[R_{t+1}+\gamma (R_{t+2}+\gamma R_{t+3}...)]=E_{\pi}[R_{t+1}+\gamma G_{t+1}]=E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})]
$$
    

    
    상태 가치 함수는 이론적으로 가장 기본적인 가치 함수. 그러나 실질적 이용 빈도는 행동 가치 함수가 더 높음. 행동 가치 함수는 특정 상태에서 특정 행동을 했을 때의 가치를 알 수 있기 때문에 강화학습의 목적(특정 상태에서 어떤 행동을 하는 것이 유리한 지를 알아내기)에 더 부합+모델이 정의되어 있지 않아도 사용할 수 있음
    

### 벨만 기대 방정식

상태 가치 함수와 행동 가치 함수 모두 재귀적인(어떤 사건이 자기 자신을 포함하고 다시 자기 자신을 이용해 계산할 때)형태로 표현 가능. 이로써 상태 $s$의 가치 함수($v_{\pi}(s)$)를 구할 때 상태  $s$ 이후의 모든 보상이 아니라 다음 단계 $s'$의 가치 함수만을 알면 되기 때문에 문제가 단순해짐. 한 번에 모든 것을 계산하는 것이 아니라 값을 변수에 저장하고 루프를 도는 방식을 통해 참 값을 알아냄. 이 재귀로 표현된 식이 바로 **벨만 방정식!**


1. **상태 가치 함수의 벨만 기대 방정식**
    
$$
v_{\pi}(s)=\sum_{a\in A}\pi(a|s)q_{\pi}(s,a)=\sum_{a\in A}\pi(a|s)\sum_{r,s'}P(s',r|s,a)[r+\gamma v_{\pi}({s'})]
$$
    
2. **큐함수(행동 가치 함수)의 벨만 기대 방정식**

$$
q_{\pi}(s,a)=r(s,a)+\gamma \sum_{s'\in S}P_{ss'}^{a}v_{\pi}(s')=\sum_{r,s'}P(s',r'|s,a)[r+\gamma \sum_{a'}\pi(a'|s')q_{\pi}(s',a')]
$$

$r(s,a)$: 행동했을 때 즉시 얻는 보상

$P_{ss'}^{a}$: $s$에서 $a$를 취할 때 $s'$으로 전이될 확률(상태 전이 확률)

$v_{\pi}(s')$: $s'$의 가치


벨만 방정식은 가치 함수의 해(참값)를 구하는 방법론임. 이 방정식은 어떤 상태 $S_t$의 상태 가치 함수와 이어지는 상태의 값 $S_{t+1}$의 상태 가치 함수 사이의 관계를 표현하고 있음→$v_{\pi}(s)$는 $s$의 주변 상태인 $s'$으로 구할 수 있음

학습 초기 에이전트는 제대로 된 정책을 갖고 있지 않아 임의의 가치 함수를 갖게 되고 좌변과 우변(상태 $s$의 가치)가 동일하지 않음. 그러나 여러 에피소드를 반복하며 학습하다 보면 양변 사이의 균형이 맞게 되고 좌변과 우변이 동일하게 됨. 이렇게 모든 상태에서 균형이 안정된 가치함수를 **참 가치 함수**라고 함. 참가치 함수는 어떤 정책에 따라 움직였을 경우에 받게 되는 보상의 참값

### 최적 정책(Optimal Policy)

최적 정책 $\pi_{*}$은 가장 큰 가치 함수를 주는 정책이며 강화학습의 최종적 학습 목표값

이때 MDP 내의 모든 $\pi$에 대해 $\pi_{*}>\pi$ 만족하는 최적정책이 반드시 존재함→어떤 MDP라도 최적의 정책이 존재함!

**최적 가치 함수**는 최적 정책을 사용될 때의 가치 함수→벨만 최적 방정식을 통해 구할 수 있음

- 최적 상태 가치 함수: $v_{*}(s)=max_{\pi}v_{\pi}(s)→v_{\pi}(s)$의 최대를 반환
- 최적 행동 가치 함수: $q_{*}(s,a)=max_{\pi}q_{\pi}(s,a)→q_{\pi}(s,a)$의 최대를 반환


정책은 상태와 행동에 관한 지표이기 때문에 최적 정책을 판단할 때에는 큐함수(최적 행동 가치 함수)가 사용됨. 최적 정책은 각 상태 $s$에서 가장 큰 큐함수를 가진 행동을 하는 것으로, 최적 큐함수 값을 알고 있다면 쉽게 구할 수 있음

$$
optimal p(s,a) =
\begin{cases} 
1 & \text{if } a = \arg\max\limits_{a \in A} q_{*}(s,a) \\
0 & \text{otherwise}
\end{cases}
$$

$argmax$는 $q_{*}(s,a)$를 최대화하는 행동 $a$를 반환. 벨만 기대 방정식에서는 $\pi(a|s)$가 곱해졌었지만, 벨만 최적 방정식에서는 최댓값 함수를 통해 최적의 행동을 선택하는 경우만 따지는 것이기 때문에 곱할 필요가 없음

![Untitled 5](https://github.com/user-attachments/assets/ec1b7896-6e5d-4635-953a-6b6f04c12f95)

(a)는 최적 상태 가치 함수를 구하기 위한 벨만 최적 방정식을 표현한 그림. 최적의 큐함수 중에서 max를 선택하는 것이 최적의 가치함수가 됨


$$
v_{opt}(s) = \max_{a} q_{opt}(s,a)
$$

(b)는 최적 행동 가치 함수를 구하기 위한 벨만 최적 방정식을 표현한 그림입니다.

$$
q_{opt}(s,a) = r(s,a) + \gamma \sum_{s' \in S} P_{ss'}^{a} v_{opt}(s')
$$

### 벨만 방정식을 통한 MDP 해결

벨만 방정식을 통해 MDP 문제를 해결하는 것은 수학적으로/이론적으로 빈틈 없는 탐색임. 그러나 모든 경우의 수를 내다 봐야 하기 때문에 효용이 낮음. 만약 보상 함수와 전이 확률을 안다면 “MDP를 안다”고 표현하지만 모른다면 “MDP를 모른다”고 표현. 또한 다음 세 가지 조건을 만족시켜야지만 계산이 가능함(→다이나믹 프로그래밍)

1. 환경의 구조를 명확히 알아야 함
2. 전체를 계산할 만큼 충분한 계산 실력이 있어야 함
3. 마르코프 성질을 가정해야 함

그러나 현실에선 이 조건을 모두 만족하는 것은 불가능에 가깝기 때문에 근사적인 해를 찾는 방향으로 강화학습은 발전하고 있음. 일반적으로 MDP에 대해 모르는 상황이 더 많기 때문에 Model-Free 접근법을 사용해야 함


## 정리

- **MDP**
    
    순차적 행동 결정 문제를 수학적으로 정의한 것. 상태($S$), 행동($A$), 보상함수($R$), 상태 전이 확률($P$), 할인율($\gamma$)로 구성되어 있음. 순차적 행동 결정 문제를 푸는 것은 더 좋은 정책을 찾는 과정
    
- **가치 함수**
    
    에이전트가 어떤 정책이 더 좋은지 판단하는 기준. 가치 함수는 정책을 따라갔을 때 받을 수 있으리라 기대하는 보상의 총합의 기댓값. 가치 함수는 행동의 가치를 직접적으로 나타내는 큐함수를 사용
    
- **벨만 방정식**
    
    현재 상태의 가치함수와 다음 상태 가치함수와의 관계를 나타낸 것
    
    벨만 기대 방정식: 특정 정책을 따라갔을 때 가치함수 사이의 관계식
    
    벨만 최적 방정식: 최적 정책을 따라갔을 때 가치함수 사이의 관계식
    


## 1주차 문제

1. 정책의 정의: 특정 상태에서 어떤 행동을 해야할 지 알려주는 지표
2. 정책은 탐욕적이다. 탐욕적인 것(greedy)는 무엇인가?
    
    -선택을 할 때, 주어진 선택지에서 가장 큰 값을 선택하는 것이다. 강화학습에 있어 정책이 탐욕적이라는 것은 모든 상태마다 얻을 수 있는 가치가 가장 높은 행동을 선택하는 것을 의미한다. 따라서 단기적으로는 가장 큰 이득을 얻을 수 있고, 장기적으로 봤을 때는 최적 정책에 도달할 수 있다.  
    
3. 행동 가치 함수와 상태 가치 함수의 차이점에 대해 서술하시오.
    
    -상태 가치 함수는 가치를 구할 때 상태만을 고려하기 때문에 MDP 상황이 정의된 모델이 필요하지만 행동 가치 함수는 상태와 행동을 모두 고려해 특정 상황에 대한 행동의 가치를 알 수 있기 때문에 따로 모델이 정의되어 있지 않아도 사용할 수 있다.
    
4. 보상과 가치의 차이점
    
    -보상은 특정 상태에 대해 어떤 행동을 하면 즉각적으로 제공되는 이득이다. 가치는 일정 시간 동안 주어지는 보상의 총합이며, 강화학습에서 궁극적으로 최대화해야 할 대상이다.  
    
5. 참가치 함수와 최적 가치 함수는 동일하다(o/x)
    
    -x/참가치 함수는 어떤 정책을 따랐을 때의 받게 되는 보상의 참 값이다. 그러나 최적 가치 함수는 최적의 정책을 따랐을 때, 즉 가장 높은 보상을 얻게 되는 정책을 따랐을 때의 가치함수이다.
    
6. 벨만 기대 방정식과 벨만 최적 방정식의 차이점
    
    -벨만 기대 방정식은 특정 정책을 따라갔을 때 가치함수 사이의 관계식이며, 벨만 최적 방정식은 최적의 정책을 따라갔을 때 가치함수 사이의 관계식이다. 또한 벨만 기대 방정식은 계산할 때 상태 $s$에서 행동 $a$를 취할 확률인 정책$(\pi(a|s))$가 필요하지만 벨만 최적 방정식은 최댓값 연산자를 통해 최적의 정책만을 선택하기 때문에 계산에 정책이 반영되지 않는다. 
    
7. 행동 가치 함수 공식 유도 직접 해보기

![IMG_5073](https://github.com/user-attachments/assets/81c9ed67-b6dc-487c-a058-c77969b4ef8a)
