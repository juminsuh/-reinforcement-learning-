{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 지뢰찾기\n",
        "\n",
        ">state: 타일 하나가 가질 수 있는 상태\n",
        "\n",
        "\n",
        "*   [-1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
        "\n",
        ">action: open tile\n",
        "\n",
        "\n",
        "\n",
        "*   [(0, 0), (0, 1)...(0, 8),\n",
        "(1, 0), (1, 1)...(1, 8), ...(8,8)]: 9 x 9 = 81개의 action or [0, 1, 2, ...80]까지 indexing\n",
        "\n",
        "> reward\n",
        "\n",
        "\n",
        "\n",
        "*   -1 if (state == -1) 1 elif (state == 0) 0 else\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UEuaqkUQYz7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "uZo3yvvNRuiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "Dh8itAsTRuND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Env"
      ],
      "metadata": {
        "id": "1MXkY1IZ3Mhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "devices = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(devices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WOfNgU0zo6r",
        "outputId": "a694ee92-97c4-46da-d91a-da360f5c09bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MinesweeperEnv:\n",
        "    def __init__(self, height=9, width=9, num_mines=10):\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.num_mines = num_mines\n",
        "        self.minefield = np.zeros((self.height, self.width), dtype=int)  # Initialize minefield\n",
        "        self.initialize_minefield()\n",
        "        self.reward = {'lose': -10, '0': 5, 'general': 3, 'win': 10}\n",
        "\n",
        "    def initialize_minefield(self):\n",
        "        # Reset minefield and place mines\n",
        "        self.minefield = np.zeros((self.height, self.width), dtype=int)\n",
        "        mines = set()\n",
        "        while len(mines) < self.num_mines:\n",
        "            x, y = random.randint(0, self.height - 1), random.randint(0, self.width - 1)\n",
        "            if (x, y) not in mines:\n",
        "                mines.add((x, y))\n",
        "                self.minefield[x, y] = -1\n",
        "\n",
        "        # Calculate numbers\n",
        "        for (x, y) in mines:\n",
        "            for i in range(max(0, x - 1), min(self.height, x + 2)):\n",
        "                for j in range(max(0, y - 1), min(self.width, y + 2)):\n",
        "                    if self.minefield[i, j] != -1:\n",
        "                        self.minefield[i, j] += 1\n",
        "\n",
        "    def reset(self, restrict=False):\n",
        "        if not restrict:\n",
        "            self.initialize_minefield()  # Ensure minefield is initialized if not restricted\n",
        "\n",
        "        self.playerfield = np.full((self.height, self.width), 9)  # Reset playerfield\n",
        "        self.exploded = False\n",
        "        self.done = False\n",
        "\n",
        "        if not restrict:\n",
        "            # Uncover a safe cell to start\n",
        "            while True:\n",
        "                sx, sy = random.randint(0, self.height - 1), random.randint(0, self.width - 1)\n",
        "                if self.minefield[sx, sy] != -1:\n",
        "                    self.playerfield = self._uncover(sx, sy)\n",
        "                    if self.playerfield[sx, sy] == 0:\n",
        "                        self.auto_reveal_blocks(sx, sy)\n",
        "                    break\n",
        "\n",
        "        return self.playerfield\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        # Convert action index to (x, y) coordinates\n",
        "        x, y = divmod(action_idx, self.width)\n",
        "        reward = 0\n",
        "\n",
        "        # self.playerfield = self.playerfield.reshape(self.height, self.width)\n",
        "        if self.playerfield[x, y] == 9:\n",
        "            self.playerfield = self._uncover(x, y)\n",
        "\n",
        "            if self.playerfield[x, y] == -1:\n",
        "                self.exploded = True\n",
        "                self.done = True\n",
        "                reward = self.reward['lose']\n",
        "            elif self.playerfield[x, y] == 0:\n",
        "                self.auto_reveal_blocks(x, y)\n",
        "                reward = self.reward['0']\n",
        "                self.exploded = False\n",
        "                self.done = False\n",
        "            else:\n",
        "                reward = self.reward['general']\n",
        "                self.exploded = False\n",
        "                self.done = False\n",
        "\n",
        "        # Check if all non-mine cells are uncovered\n",
        "        if not self.exploded and np.all((self.playerfield != 9) | (self.minefield == -1)):\n",
        "            self.done = True\n",
        "            reward = self.reward['win']\n",
        "\n",
        "        return self.playerfield, reward, self.exploded, self.done\n",
        "\n",
        "    def _uncover(self, x, y):\n",
        "        self.playerfield[x, y] = self.minefield[x, y]\n",
        "        return self.playerfield\n",
        "\n",
        "    def auto_reveal_blocks(self, x, y):\n",
        "        queue = [(x, y)]\n",
        "        while queue:\n",
        "            cx, cy = queue.pop(0)\n",
        "            for i in range(max(0, cx - 1), min(self.height, cx + 2)):\n",
        "                for j in range(max(0, cy - 1), min(self.width, cy + 2)):\n",
        "                    if self.playerfield[i, j] == 9:\n",
        "                        self.playerfield = self._uncover(i, j)\n",
        "                        if self.minefield[i, j] == 0:\n",
        "                            queue.append((i, j))\n",
        "\n",
        "    def render(self):\n",
        "        render_str = ''\n",
        "        for x in range(self.height):\n",
        "            for y in range(self.width):\n",
        "                if self.playerfield[x, y] == 9:\n",
        "                    render_str += '. '  # H for hidden\n",
        "                elif self.playerfield[x, y] == -1:\n",
        "                    render_str += 'M '  # M for mine\n",
        "                else:\n",
        "                    render_str += f'{self.playerfield[x, y]} '\n",
        "            render_str += '\\n'\n",
        "        print(render_str)\n"
      ],
      "metadata": {
        "id": "vXBnha4fJDWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_multiple_boards(num_boards, height=9, width=9, num_mines=10):\n",
        "    boards = []\n",
        "    for _ in range(num_boards):\n",
        "            env = MinesweeperEnv(height, width, num_mines)\n",
        "            boards.append(env)\n",
        "    return boards"
      ],
      "metadata": {
        "id": "_2N9BqW_k33_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network\n",
        "\n",
        "나중에 보강"
      ],
      "metadata": {
        "id": "QsRfnP8gRZzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "\n",
        "    # state: 타일의 상태\n",
        "    # input state: 타일의 상태가 모인 전체 게임판(player)\n",
        "    # state_size.shape = (batch_size, 9, 9) -> 각 칸들은 다른 state를 가짐\n",
        "    # action_size.shape = (batch_size, 81)\n",
        "    # 상태가 들어오면 게임판 좌표 중 행동을 nn으로 근사\n",
        "    # NeuralNet에 들어가는 모든 값들은 tensor\n",
        "    def __init__(self, output):\n",
        "\n",
        "        super(NeuralNet, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size = 3, stride = 1, padding = 1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(128)\n",
        "        self.maxpool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(128*2*2, 128)\n",
        "        self.fc2 = nn.Linear(128, output) # 81개의 행동에 대한 확률 출력\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 128*2*2)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "bLVByi8SRbpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = torch.randn(1, 1, 9, 9)\n",
        "neuralnetwork = NeuralNet(81)\n",
        "output = neuralnetwork(input_data)\n",
        "print(output.shape)\n",
        "print(output.sum(dim = 1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qPNmWhk5O8-",
        "outputId": "7976c70d-b4e2-4945-ba3b-e55c7d8a608e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 81])\n",
            "tensor([-4.5524], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agent\n",
        "> __init__\n",
        "\n",
        "\n",
        "*   state_size, action_size\n",
        "*   memory_size, memory_size_min, batch_size, memory\n",
        "*  discount_factor, epsilon, epsilon_min, epsilon_decay\n",
        "*  learning_rate, loss, model, target_model, optimizer\n",
        "* update_target_model()\n",
        "\n",
        "> method\n",
        "\n",
        "\n",
        "*   update_target_model(self)\n",
        "*   get_action(self, state)\n",
        "* append_sample(self, state, action, reward, next_state, done)\n",
        "* train_model(self)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ly1LESFuQobd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter"
      ],
      "metadata": {
        "id": "v_Iu0FQBbl2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 16\n",
        "MEMORY_SIZE = 1000\n",
        "MEMORY_SIZE_MIN = 50\n",
        "\n",
        "EPSILON = 0.9\n",
        "EPSILON_DECAY = 0.999\n",
        "EPSILON_MIN = 0.01\n",
        "DISCOUNT_FACTOR = 0.1\n",
        "\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "UPDATE_TIME = 20"
      ],
      "metadata": {
        "id": "rMGjFDoYblsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdyMgiBWLQTg"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # memory\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        self.memory_size = MEMORY_SIZE\n",
        "        self.memory_size_min = MEMORY_SIZE_MIN\n",
        "        self.memory = deque(maxlen = self.memory_size)\n",
        "\n",
        "        # epsilon\n",
        "        self.epsilon = EPSILON\n",
        "        self.epsilon_decay = EPSILON_DECAY\n",
        "        self.epsilon_min = EPSILON_MIN\n",
        "        self.discount_factor = DISCOUNT_FACTOR\n",
        "\n",
        "        # cpu -> gpu\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # 인공신경망 model, 타깃신경망 target_model\n",
        "        self.learning_rate = LEARNING_RATE\n",
        "        self.loss = nn.MSELoss()\n",
        "        self.model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.target_model = NeuralNet(self.action_size).to(self.device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
        "\n",
        "        # 일정 time step(한 episode)이 지나면 target 신경망을 인공신경망으로 대체\n",
        "        self.update_time = UPDATE_TIME\n",
        "        self.update_target_model()\n",
        "\n",
        "    # 인공신경망의 파라미터 -> target model의 파라미터\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    # 엡실론 - 탐욕 정책\n",
        "    def get_action(self, state):\n",
        "\n",
        "\n",
        "        random_prob = np.random.rand()\n",
        "        if random_prob <= self.epsilon:\n",
        "            random_list = []\n",
        "            random_action = random.randrange(self.action_size)\n",
        "            random_list.append(random_action)\n",
        "            x, y = divmod(random_action, 9)\n",
        "            while (state[x, y] != 9):\n",
        "                 available_list = [action for action in list(range(self.action_size)) if action not in random_list]\n",
        "                 random_action = random.choice(available_list)\n",
        "                 random_list.append(random_action)\n",
        "                 x, y = divmod(random_action, 9)\n",
        "\n",
        "            return random_action\n",
        "\n",
        "        else:\n",
        "            state = torch.tensor(state, dtype = torch.float32).to(self.device)\n",
        "            # state가 주어졌을 때의 행동별 q_value\n",
        "            q_value = self.model(state.unsqueeze(0).unsqueeze(0))\n",
        "\n",
        "            # dim = 1: 행에서 최대값을 찾음\n",
        "            _, max_q_idx = torch.max(q_value, dim = 1)\n",
        "            max_q_idx = max_q_idx.item()\n",
        "            x, y = divmod(max_q_idx, 9)\n",
        "\n",
        "            while state[x, y] != 9:\n",
        "                # 선택된 타일의 q_value를 최소화함\n",
        "                min_q_value, _ = torch.min(q_value, dim = 1)\n",
        "                min_q_value = min_q_value.item()\n",
        "                q_value[0, max_q_idx] = min_q_value\n",
        "\n",
        "                _, max_q_idx = torch.max(q_value, dim = 1)\n",
        "                max_q_idx = max_q_idx.item()\n",
        "                x, y = divmod(max_q_idx, 9)\n",
        "\n",
        "            # 최대 큐함수를 갖는 행동의 인덱스\n",
        "            return max_q_idx\n",
        "\n",
        "    def append_sample(self, state, action, next_state, reward, exploded, done):\n",
        "        # done: 게임이 끝났으면 True\n",
        "        # 이때 reward를 다르게 줌\n",
        "        self.memory.append((state, action, next_state, reward, exploded, done))\n",
        "\n",
        "    def train_model(self):\n",
        "\n",
        "        self.model.train()\n",
        "        # self.target_model.eval()\n",
        "\n",
        "        # 엡실론 업데이트\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "        states, actions, next_states, rewards, explodeds, dones = zip(*mini_batch)\n",
        "\n",
        "        states = torch.tensor(states, dtype = torch.float32).unsqueeze(1).to(self.device)\n",
        "        actions = torch.tensor(actions, dtype = torch.int64).to(self.device)\n",
        "        next_states = torch.tensor(next_states, dtype = torch.float32).unsqueeze(1).to(self.device)\n",
        "        rewards = torch.tensor(rewards, dtype = torch.int64).to(self.device)\n",
        "        explodeds = torch.tensor(explodeds, dtype = torch.int64).to(self.device)\n",
        "        dones = torch.tensor(dones, dtype = torch.int64).to(self.device)\n",
        "\n",
        "        # 현재 상태에 대한 action별 큐함수(인공신경망의 예측)\n",
        "        predicts = self.model(states)\n",
        "        predicts = torch.squeeze(predicts).to(self.device)\n",
        "        # one_hot_action.shape: (16, 1)을 (16, 81)로 one-hot encoding\n",
        "        one_hot_action = F.one_hot(actions, self.action_size).to(self.device)\n",
        "        # predicts와 one_hot_action을 element-wise하게 곱하면 최대 큐함수를 갖는 행동의 큐함수만 남음 -> 상태별 최대 큐함수를 구할 수 있음\n",
        "        one_hot_pred = torch.sum(predicts * one_hot_action, dim = 1).to(self.device)\n",
        "\n",
        "        # 다음 상태에 대한 target 신경망의 action별 큐함수\n",
        "        # detach(): target model의 큐함수인 target_pred가 업데이트되지 않도록 함\n",
        "\n",
        "        with torch.no_grad():\n",
        "            target_pred, _ = torch.max(torch.squeeze(self.target_model(next_states)), dim = 1)\n",
        "\n",
        "        # 벨만 최적 방정식\n",
        "        targets = rewards + (1 - dones) * self.discount_factor * target_pred\n",
        "        loss = self.loss(one_hot_pred, targets)\n",
        "\n",
        "        # 가중치 업데이트\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# main"
      ],
      "metadata": {
        "id": "H7xubXmC3ncy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the number of boards and create them\n",
        "num_boards = 10\n",
        "boards = create_multiple_boards(num_boards)\n",
        "\n",
        "board = boards[0]\n",
        "agent = Agent(board.height * board.width, board.height * board.width)\n",
        "\n",
        "# Define the number of episodes\n",
        "EPISODES = 10000\n",
        "\n",
        "# episode, 총 score, score_avg, 게임이 끝날 때까지의 action 수를 저장할 리스트\n",
        "episodes, scores, score_avg, total_action, length_memory, win_list = [], [], [], [], [], []\n",
        "\n",
        "\n",
        "for epi in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "    total_action_epi = []\n",
        "\n",
        "    # Use the fixed board\n",
        "    # env = board\n",
        "    # Use the random board\n",
        "    env = random.choice(boards)\n",
        "\n",
        "    # 게임 다시 시작; 게임판 초기화\n",
        "    state = env.reset(restrict=True) # 일단은 판을 고정하고, 나중에 할 때는 False로 바꿔서 계속 판이 업데이트된다.\n",
        "    while not done:\n",
        "        # 현재 상태를 기반으로 행동을 선택\n",
        "        action = agent.get_action(state)\n",
        "        total_action_epi.append(action)\n",
        "        next_state, reward, exploded, done = env.step(action)\n",
        "        score += reward\n",
        "        agent.append_sample(state, action, next_state, reward, exploded, done)\n",
        "\n",
        "        # memory의 길이가 최소 이상일 때 훈련\n",
        "        # batch_size만큼의 데이터가 훈련\n",
        "        if len(agent.memory) > agent.memory_size_min:\n",
        "            agent.train_model()\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # win\n",
        "            if not env.exploded:\n",
        "                win_list.append(1)\n",
        "            else:\n",
        "                win_list.append(0)\n",
        "\n",
        "             # agent.update_time마다 target_model의 가중치 업데이트\n",
        "            if (epi % agent.update_time) == 0:\n",
        "                agent.update_target_model()\n",
        "\n",
        "            total_action.append(len(total_action_epi))\n",
        "            episodes.append(epi)\n",
        "            scores.append(score)\n",
        "            score_avg.append(np.mean(scores[-50:]))\n",
        "            length_memory.append(len(agent.memory))\n",
        "            '''\n",
        "            print(f\"episode: {epi} | epsilon: {agent.epsilon} | num_action: {total_action[-1]} | score: {scores[-1]} | length of memory: {length_memory[-1]}\")\n",
        "            print(f\"action_list: {total_action_epi}\")\n",
        "            env.render()\n",
        "            '''\n",
        "\n",
        "            if (epi % 50) == 0:\n",
        "                win_rate = np.mean(win_list[-50:]) if len(win_list) >= 50 else np.mean(win_list) # -50이므로 마지막에서 50번째 최신 것을 반영함.\n",
        "                print(f\"Episode: {epi} | Score: {np.mean(scores[-50:])} | Epsilon: {agent.epsilon:.2f} | Win Rate: {win_rate}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBDcvCmKJHQK",
        "outputId": "15d332ce-1197-4bf1-ef1d-bcc6fe37a90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 0 | Score: -10.0 | Epsilon: 0.90 | Win Rate: 0.0\n",
            "Episode: 50 | Score: 4.72 | Epsilon: 0.74 | Win Rate: 0.0\n",
            "Episode: 100 | Score: 1.4 | Epsilon: 0.60 | Win Rate: 0.0\n",
            "Episode: 150 | Score: 9.7 | Epsilon: 0.44 | Win Rate: 0.0\n",
            "Episode: 200 | Score: 12.48 | Epsilon: 0.31 | Win Rate: 0.02\n",
            "Episode: 250 | Score: 23.98 | Epsilon: 0.18 | Win Rate: 0.06\n",
            "Episode: 300 | Score: 26.44 | Epsilon: 0.11 | Win Rate: 0.24\n",
            "Episode: 350 | Score: 48.14 | Epsilon: 0.05 | Win Rate: 0.54\n",
            "Episode: 400 | Score: 48.04 | Epsilon: 0.03 | Win Rate: 0.62\n",
            "Episode: 450 | Score: 57.4 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 500 | Score: 58.2 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 550 | Score: 59.74 | Epsilon: 0.01 | Win Rate: 0.94\n",
            "Episode: 600 | Score: 63.22 | Epsilon: 0.01 | Win Rate: 0.94\n",
            "Episode: 650 | Score: 60.22 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 700 | Score: 56.54 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 750 | Score: 58.48 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 800 | Score: 48.2 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 850 | Score: 53.66 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 900 | Score: 60.4 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 950 | Score: 58.94 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 1000 | Score: 48.42 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 1050 | Score: 34.42 | Epsilon: 0.01 | Win Rate: 0.54\n",
            "Episode: 1100 | Score: 33.72 | Epsilon: 0.01 | Win Rate: 0.62\n",
            "Episode: 1150 | Score: 41.1 | Epsilon: 0.01 | Win Rate: 0.66\n",
            "Episode: 1200 | Score: 40.26 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 1250 | Score: 51.3 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 1300 | Score: 19.16 | Epsilon: 0.01 | Win Rate: 0.36\n",
            "Episode: 1350 | Score: 17.26 | Epsilon: 0.01 | Win Rate: 0.38\n",
            "Episode: 1400 | Score: 26.98 | Epsilon: 0.01 | Win Rate: 0.5\n",
            "Episode: 1450 | Score: 19.52 | Epsilon: 0.01 | Win Rate: 0.4\n",
            "Episode: 1500 | Score: 30.68 | Epsilon: 0.01 | Win Rate: 0.6\n",
            "Episode: 1550 | Score: 23.6 | Epsilon: 0.01 | Win Rate: 0.46\n",
            "Episode: 1600 | Score: 8.94 | Epsilon: 0.01 | Win Rate: 0.28\n",
            "Episode: 1650 | Score: 29.04 | Epsilon: 0.01 | Win Rate: 0.58\n",
            "Episode: 1700 | Score: 31.18 | Epsilon: 0.01 | Win Rate: 0.56\n",
            "Episode: 1750 | Score: 29.18 | Epsilon: 0.01 | Win Rate: 0.58\n",
            "Episode: 1800 | Score: 30.26 | Epsilon: 0.01 | Win Rate: 0.6\n",
            "Episode: 1850 | Score: 38.04 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 1900 | Score: 37.88 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 1950 | Score: 38.48 | Epsilon: 0.01 | Win Rate: 0.7\n",
            "Episode: 2000 | Score: 35.88 | Epsilon: 0.01 | Win Rate: 0.64\n",
            "Episode: 2050 | Score: 26.5 | Epsilon: 0.01 | Win Rate: 0.48\n",
            "Episode: 2100 | Score: 43.04 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 2150 | Score: 28.96 | Epsilon: 0.01 | Win Rate: 0.58\n",
            "Episode: 2200 | Score: 23.04 | Epsilon: 0.01 | Win Rate: 0.5\n",
            "Episode: 2250 | Score: 38.88 | Epsilon: 0.01 | Win Rate: 0.6\n",
            "Episode: 2300 | Score: 36.3 | Epsilon: 0.01 | Win Rate: 0.62\n",
            "Episode: 2350 | Score: 34.68 | Epsilon: 0.01 | Win Rate: 0.64\n",
            "Episode: 2400 | Score: 37.34 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 2450 | Score: 31.22 | Epsilon: 0.01 | Win Rate: 0.58\n",
            "Episode: 2500 | Score: 37.0 | Epsilon: 0.01 | Win Rate: 0.64\n",
            "Episode: 2550 | Score: 31.88 | Epsilon: 0.01 | Win Rate: 0.54\n",
            "Episode: 2600 | Score: 30.4 | Epsilon: 0.01 | Win Rate: 0.58\n",
            "Episode: 2650 | Score: 38.42 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 2700 | Score: 37.6 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 2750 | Score: 36.24 | Epsilon: 0.01 | Win Rate: 0.68\n",
            "Episode: 2800 | Score: 33.64 | Epsilon: 0.01 | Win Rate: 0.64\n",
            "Episode: 2850 | Score: 43.18 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 2900 | Score: 43.28 | Epsilon: 0.01 | Win Rate: 0.62\n",
            "Episode: 2950 | Score: 37.46 | Epsilon: 0.01 | Win Rate: 0.5\n",
            "Episode: 3000 | Score: 45.2 | Epsilon: 0.01 | Win Rate: 0.54\n",
            "Episode: 3050 | Score: 49.0 | Epsilon: 0.01 | Win Rate: 0.7\n",
            "Episode: 3100 | Score: 53.84 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 3150 | Score: 56.2 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 3200 | Score: 54.1 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 3250 | Score: 59.58 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 3300 | Score: 55.64 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 3350 | Score: 59.2 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 3400 | Score: 51.94 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 3450 | Score: 59.82 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 3500 | Score: 45.48 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 3550 | Score: 58.44 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 3600 | Score: 58.6 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 3650 | Score: 52.38 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 3700 | Score: 55.58 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 3750 | Score: 48.96 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 3800 | Score: 57.88 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 3850 | Score: 60.04 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 3900 | Score: 60.48 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 3950 | Score: 59.06 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 4000 | Score: 54.9 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 4050 | Score: 59.2 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 4100 | Score: 59.2 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 4150 | Score: 56.58 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 4200 | Score: 60.54 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 4250 | Score: 57.5 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 4300 | Score: 54.82 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 4350 | Score: 64.0 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 4400 | Score: 61.44 | Epsilon: 0.01 | Win Rate: 0.96\n",
            "Episode: 4450 | Score: 57.78 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 4500 | Score: 57.04 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 4550 | Score: 58.54 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 4600 | Score: 57.98 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 4650 | Score: 59.92 | Epsilon: 0.01 | Win Rate: 0.94\n",
            "Episode: 4700 | Score: 47.1 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 4750 | Score: 45.02 | Epsilon: 0.01 | Win Rate: 0.66\n",
            "Episode: 4800 | Score: 57.16 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 4850 | Score: 52.42 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 4900 | Score: 50.12 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 4950 | Score: 56.5 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 5000 | Score: 57.84 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 5050 | Score: 56.38 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 5100 | Score: 58.86 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 5150 | Score: 51.98 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 5200 | Score: 46.28 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 5250 | Score: 58.56 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 5300 | Score: 58.7 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 5350 | Score: 55.36 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 5400 | Score: 55.68 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 5450 | Score: 54.98 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 5500 | Score: 57.6 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 5550 | Score: 60.18 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 5600 | Score: 61.92 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 5650 | Score: 53.96 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 5700 | Score: 60.56 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 5750 | Score: 54.32 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 5800 | Score: 52.0 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 5850 | Score: 48.7 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 5900 | Score: 55.64 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 5950 | Score: 57.1 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 6000 | Score: 57.48 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 6050 | Score: 53.08 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 6100 | Score: 50.54 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 6150 | Score: 58.38 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 6200 | Score: 57.14 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 6250 | Score: 50.64 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 6300 | Score: 57.18 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 6350 | Score: 47.22 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 6400 | Score: 58.16 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 6450 | Score: 57.5 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 6500 | Score: 57.24 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 6550 | Score: 53.06 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 6600 | Score: 52.08 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 6650 | Score: 49.66 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 6700 | Score: 60.24 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 6750 | Score: 55.58 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 6800 | Score: 53.44 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 6850 | Score: 54.26 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 6900 | Score: 53.14 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 6950 | Score: 55.76 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 7000 | Score: 46.98 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 7050 | Score: 56.8 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7100 | Score: 59.42 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 7150 | Score: 57.94 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 7200 | Score: 60.12 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 7250 | Score: 56.36 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 7300 | Score: 51.48 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7350 | Score: 61.46 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 7400 | Score: 50.56 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7450 | Score: 53.74 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7500 | Score: 52.32 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 7550 | Score: 53.58 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7600 | Score: 62.44 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 7650 | Score: 57.72 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 7700 | Score: 56.56 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 7750 | Score: 57.54 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 7800 | Score: 56.38 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 7850 | Score: 55.02 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 7900 | Score: 58.68 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 7950 | Score: 62.46 | Epsilon: 0.01 | Win Rate: 0.96\n",
            "Episode: 8000 | Score: 61.72 | Epsilon: 0.01 | Win Rate: 0.94\n",
            "Episode: 8050 | Score: 63.04 | Epsilon: 0.01 | Win Rate: 0.96\n",
            "Episode: 8100 | Score: 59.22 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 8150 | Score: 50.26 | Epsilon: 0.01 | Win Rate: 0.7\n",
            "Episode: 8200 | Score: 61.7 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 8250 | Score: 61.36 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 8300 | Score: 59.52 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 8350 | Score: 57.32 | Epsilon: 0.01 | Win Rate: 0.92\n",
            "Episode: 8400 | Score: 57.48 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 8450 | Score: 66.18 | Epsilon: 0.01 | Win Rate: 0.94\n",
            "Episode: 8500 | Score: 61.06 | Epsilon: 0.01 | Win Rate: 0.88\n",
            "Episode: 8550 | Score: 56.82 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 8600 | Score: 59.84 | Epsilon: 0.01 | Win Rate: 0.9\n",
            "Episode: 8650 | Score: 50.94 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 8700 | Score: 47.56 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 8750 | Score: 49.08 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 8800 | Score: 55.16 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 8850 | Score: 56.6 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 8900 | Score: 54.62 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 8950 | Score: 54.8 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 9000 | Score: 50.3 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 9050 | Score: 49.9 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 9100 | Score: 54.52 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 9150 | Score: 55.84 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 9200 | Score: 52.24 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 9250 | Score: 49.62 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 9300 | Score: 48.72 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 9350 | Score: 46.72 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 9400 | Score: 48.26 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 9450 | Score: 50.36 | Epsilon: 0.01 | Win Rate: 0.76\n",
            "Episode: 9500 | Score: 42.12 | Epsilon: 0.01 | Win Rate: 0.66\n",
            "Episode: 9550 | Score: 57.58 | Epsilon: 0.01 | Win Rate: 0.84\n",
            "Episode: 9600 | Score: 46.22 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 9650 | Score: 58.02 | Epsilon: 0.01 | Win Rate: 0.74\n",
            "Episode: 9700 | Score: 49.92 | Epsilon: 0.01 | Win Rate: 0.72\n",
            "Episode: 9750 | Score: 57.58 | Epsilon: 0.01 | Win Rate: 0.86\n",
            "Episode: 9800 | Score: 53.2 | Epsilon: 0.01 | Win Rate: 0.8\n",
            "Episode: 9850 | Score: 52.12 | Epsilon: 0.01 | Win Rate: 0.78\n",
            "Episode: 9900 | Score: 50.34 | Epsilon: 0.01 | Win Rate: 0.82\n",
            "Episode: 9950 | Score: 48.12 | Epsilon: 0.01 | Win Rate: 0.74\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vmk99o9lEOlW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}