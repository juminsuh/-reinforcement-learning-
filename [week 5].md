# 5주차_강화학습

태그: 완료

## Q-learning→DQN

Q-learning은 각 상태에 따른 행동 가치를 테이블 형식으로 저장하는 강화학습 방법론이다. 그러나 테이블 형식은 에피소드가 무한하거나 길 때 사용이 제한된다. DQN(Deep Q-Learning)은 인공 신경망을 이용해 큐함수에 근사하여 테이블 형식의 한계를 극복했다. 

![Untitled](https://github.com/user-attachments/assets/43d96949-3594-4a85-ac67-195498462604)

## DQN의 main idea: Replay Memory

DQN은 경험 리플레이를 한다는 특징이 있다. 경험 리플레이란 리플레이 메모리에 에이전트가 환경을 탐험하며 얻은 샘플 (s, a, s’, a’)를 저장한다. DQN은 리플레이 메모리에서 batch_size만큼의 샘플을 꺼내 학습시킨다. 

### 리플레이 메모리의 성질

1. **queue로 표현: FIFO 구조(First In First Out: 선입선출)**
    
    메모리가 꽉 차면 맨 처음 들어온 순서대로 데이터가 차례대로 삭제된다. 에피소드가 증가할수록 메모리에는 양질의 데이터가 저장된다. 
    
   ![Untitled 1](https://github.com/user-attachments/assets/f7068e2c-1dfc-46fe-930e-0fa1c2557bb6)
    
2. **샘플 간의 시간적 상관관계 제거**
    
    딥살사의 경우 온폴리시 정책이기 때문에 이전에 한 행동이 나쁘지 않아도 이후의 행동이 나쁜 결과를 불러오면 이전에 한 행동도 나쁘게 평가된다(온폴리시의 한계). 이는 샘플들 간의 시간적 상관관계가 있기 때문이다. 그러나 리플레이 메모리는 메모리에서 샘플들을 랜덤으로 추출하기 때문에 시간적인 상관관계에 구애받지 않는다. 
    
3. **batched data 이용**
    
    딥살사는 매 time step마다 발생하는 (s, a, r, s’, a’) 데이터로 업데이트를 진행한다. DQN은 time step마다 업데이트를 하되 리플레이 메모리에서 batch_size만큼 데이터를 꺼내오기 때문에 안정적으로 신경망을 업데이트한다. 
    

## 인공 신경망

### 오프 폴리쉬

오프 폴리쉬는 행동 정책과 목표 정책을 분리한다. 따라서 큐러닝이 인공신경망과 합쳐질 때도 행동을 결정하는 신경망과 정답을 뱉는 신경망을 분리해서 구현한다. 행동을 결정하는 신경망은 매 time step마다 업데이트된다. 정답을 뱉는 신경망은 학습되지 않고 일정한 time step 이후에 행동 결정 신경망으로 대체된다. 이러한 방식으로 오프 폴리쉬는 온폴리쉬의 한계를 극복했다. 

### 업데이트

큐러닝에서는 정답값으로  $R_{t+1}+\gamma max_{a’}Q(s’,a’,\theta^-)$를, 예측값으로 $Q(s,a,\theta)$를 사용한다. 오류함수로는 MSE(Mean Squared Error)를 사용한다. $\theta^-$는 정답 신경망의 파라미터고, $\theta$는 행동 신경망의 파라미터이다. 

가치 함수 근사 인공 신경망 업데이트 식

$$
MSE=(ans-pred)^2=( R_{t+1}+\gamma max_{a’}Q(s’,a’,\theta^-)-Q(s,a, \theta))^2
$$

![Untitled 2](https://github.com/user-attachments/assets/d2b4de6b-c54e-4d0e-ba00-e19b69d29e0a)

## Q-learning→Double Q-learning

기존의 Q-learning은 몇몇 확률론적 환경에서 제대로 동작하지 않는다. 최대 행동 가치 값이 행동 가치에 대한 최대 기댓값을 줄 거라고 과대평가되기 때문이다. 

이 문제에 대한 대안이 Double Q-learning이다. Double Q-learning은 두 개의 행동 가치 함수(큐함수)를 정의한다. 두 행동 가치 함수가 상호적으로 작용하며 업데이트되고, 이는 행동 가치 함수를 과대평가하는 문제를 해결할 수 있다. 

## DQN→DDQN

DDQN(Double Deep Q-learning)은 Q-learning과 DQN을 합친 방법론이다.

DQN에서는 다음 상태에서 최대의 행동 가치를 갖는 행동을 고른 후 행동 가치를 구한다. 

DDQN에서는 다음 상태에서 최대의 행동 가치를 갖는 행동을 반환한 뒤 다시 그 상태과 그 상태에서의 행동 가치를 구한다. 

![Untitled 3](https://github.com/user-attachments/assets/9c7152f3-68ea-404d-9157-29c643d8c0c1)

## 환경의 기본 형태

### step, reset, render는 환경에서 반드시 포함되어야 하는 메소드

[Gymnasium Documentation](https://gymnasium.farama.org/api/env/)

---

## 핵심 문제 정리

### 1주차

- 정책의 정의?
    
    어떤 상태에서 어떤 행동을 할 확률이다.
    
- 벨만 기대 방정식과 벨만 최적 방정식의 차이점은 무엇인가?
    
    벨만 기대 방정식은 현재 정책을 따라갔을 때 얻을 수 있는 총 가치함수의 기댓값을 계산하는 방정식이다. 벨만 최적 방정식은 최적 정책을 따라갔을 때 얻을 수 있는 총 가치함수의 기댓값을 계산한 방정식으로, 결정론적으로 행동이 결정되기 때문에 방정식에 정책$(\pi(a|s))$를 고려하지 않는다. 
    

### 2주차

- end-point에서의 정책은 무엇인가?
    
    end-point(종단 상태)에서는 더 이상의 행동이 필요하지 않다. 따라서 end-point에서 어떤 행동을 할 확률은 0이기 때문에 정책은 0으로 주어질 것이다. 
    
- 정책 이터레이션은 정책 평가와 정책 발전으로 이루어져 있다. 각각은 어떤 과정인가?
    
    정책 평가는 현재의 정책을 이용해 현재 상태의 가치 함수를 구하는 것이다. 정책 발전은 항상 정책 평가 이후에 이루어지며, 갱신된 가치 함수를 이용해 정책을 업데이트한다.  
    

### 3주차

- 몬테카를로, SARSA, Q-learning을 비교해 설명하시오.
    
    공통점: 세 방법론은 모두 model-free이기 때문에 MDP와는 다르게 환경에 대한 모델이 필요없다는 장점이 있다. 따라서 (상태-행동)쌍에 대한 보상을 반환한다. 
    
    차이점 및 서로가 서로를 어떻게 보완하는가? : 몬테카를로는 한 번의 에피소드가 하나의 샘플이며, 에피소드가 끝나야 가치 함수를 업데이트할 수 있다는 단점이 있다. SARSA는 매 time step마다 가치 함수를 업데이트하며  $\varepsilon-$탐욕 정책을 따라 탐험한다. 이때 에이전트의 탐험을 통해 샘플 (s, a, r, s’, a’)가 축적되며 이를 통해 가치 함수가 업데이트된다. 그러나 SARSA는 온폴리시로, 행동 정책과 목표 정책이 분리되어 있지 않아 잘못된 학습이 진행될 가능성이 있다. 이를 극복한 것이 Q-learning으로, 오프폴리시이기 때문에 행동 정책과 목표 정책이 분리되어 있다. 따라서 다음 상태로 갈 때는 $\varepsilon$-탐욕 정책을 따르면서도 그 다음 상태에서 행동을 선택할 땐 무조건 탐욕적으로 행동하며 학습한다. 
    
- $\varepsilon$-탐욕 정책이란?
    
    이 정책은 학습에 탐험의 개념을 추가한다. 에이전트가 탐험을 함으로써 훨씬 다양한 환경을 알아갈 수 있도록 하는 것이다. $\varepsilon$%의 확률로 엉뚱한 선택을 하고 ($100-\varepsilon$)%의 확률로 탐욕적으로 행동한다. 
    

### 4주차

- 딥살사 nn의 입출력은 무엇인가?
    
    입력은 상태, 출력은 큐함수이다. 딥살사의 nn은 큐함수를 근사한다. 
    
- 딥살사의 파라미터 업데이트에 대해 설명해시오.
    
    딥살사의 파라미터는 큐함수에 근사하는 인공신경망의 파라미터이다. 오류 함수는 MSE(Mean Squared Error)을 사용하며 $(ans-pred)^2$의 식으로 구한다. 경사하강법을 통해 오류함수를 최소화하는 파라미터  $\theta$를 구한다. 
    
    $MSE=(ans-pred)^2=(R_{t+1}+\gamma Q(s’,a’)-Q(s,a))^2$
    

### 5주차

- DQN의 가장 중요한 특징 중 하나는 리플레이 메모리를 사용한다는 것이다. 리플레이 메모리가 뭔지 설명하고, 특징 3가지를 설명하시오.
    
    리플레이 메모리는 Q-learning의 에이전트가 환경을 탐험하며 얻은 샘플 (s,a,r,s’)를 저장한다. 특징 첫번째는 FIFO(선입선출) 구조를 따른다는 것이며 한쪽으로는 입력밖에 못하고 다른 한쪽으로 출력밖에 하지 못한다. 따라서 에피소드가 증가함에 따라 리플레이 메모리는 양질의 메모리로 가득 차게 된다. 특징 두 번째는 샘플들 간의 시간적 상관관계가 없어진다는 것이다. 딥살사의 경우는 현재의 샘플이 다음 샘플의 결과에 큰 영향을 받았지만 리플레이 메모리에서는 batch_size만큼 랜덤으로 샘플을 추출하기 때문에 이 문제가 해결된다. 특징 세 번째는 batch_size만큼 샘플을 추출한다는 것으로, 안정적으로 신경망을 업데이트할 수 있다는 것이다. 
    
- DQN nn에 대해 설명하시오.
    
    DQN은 오프폴리시이기 때문에 nn이 정답 nn, 행동 nn, 이렇게 두 개다. 행동 nn의 파라미터는 $\theta$로 매 time step마다 업데이트된다. 정답 nn의 파라미터는 $\theta^-$로 매 time step마다 학습되지 않고 일정 time step 이후 행동 nn으로 대체된다. DQN도 MSE(Mean Squared Error)를 사용해 파라미터($\theta$)를 업데이트한다. 
    
    $MSE=(ans-pred)^2=(R_{t+1}+\gamma max_{a}Q(s’,a’,\theta^-)-Q(s,a,\theta))^2$
    
- 딥살사와 DQN은 모두 가치 그래디언트이다. (o/x)
    
    o/ 딥살사와 DQN은 모두 정책이 아닌 행동 가치 함수(큐함수)를 근사하는 가치 그래디언트이다.
