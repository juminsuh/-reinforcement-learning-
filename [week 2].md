# 2주차_강화학습

태그: 완료
주차 내용: rendering_method

## 01. 다이나믹 프로그래밍

다이나믹 프로그래밍은 하나의 큰 문제를 여러 개의 작은 문제로 나눈 후, 단계적으로 작은 문제를 해결하고 저장하며 결과적으로 큰 문제를 해결하는 방법론이다. 그러나, 다이나믹 프로그래밍은 환경에 대한 정보(상태 전이 확률과 보상)을 모두 알고 있다는 전제가 필요하며 엄청난 계산량이 요구되기 때문에 현재의 강화학습 방법론으로는 사용되지 않는다. 그럼에도 우리가 다이나믹 프로그래밍에 대해 알아야 하는 것은 현재의 강화학습 방법론은 다이나믹 프로그래밍을 기반으로 발전했기 때문이다.  다이나믹 프로그래밍을 이용해 문제를 푸는 방법론에는 **정책 이터레이션**과 **가치 이터레이션**이 있다. 

## 02. 정책 이터레이션

정책 이터레이션은 기존의 정책에 따른 가치 함수를 계산(**정책 평가**)하고, 구한 가치함수를 토대로 정책을 발전(**정책 발전**)시키는 과정을 반복하여 최적 함수를 구한다. 정책 평가 과정에서 가치 함수를 구할 때 **벨만 기대 방정식**을 사용하고 정책 발전 과정에서 정책을 구할 때 **탐욕적**으로 작용한다. 

![Untitled](https://github.com/user-attachments/assets/8f6626f7-39e5-4f6e-a09a-b749a759654f)

> $\pi_0 →^E V_{\pi_0} →^I \pi_0 →^E V_{\pi_0} →^I … → \pi_* → V_{\pi_*}$
**E : 정책 평가
I  : 정책 발전**
> 

### 정책 평가

정책 평가는 벨만 기대 방정식을 이용하여 가치 함수를 계산한다. 벨만 기대 방정식은 현재 상태에서 특정 정책을 따라갔을 때 얻을 수 있는 총보상의 기댓값으로, 특정 정책을 따라갔을 때 이어지는 두 상태 사이의 관계를 나타낸 식이다. 현 상태($s$)를 갈 수 있는 주변의 모든 상태들의 가치함수와 보상($s'$)으로 표현한다.  

> $V_{k+1} = \sum_{a \in A} \pi(a|s) \sum_ {s',r} P(s',r|a,s)[r(s,a) + \gamma V_{k}(s')]$
> 

정책 평가는 반복을 통해 벨만 기대 방정식의 등호가 성립하는 $V_{k}=V_{\pi}$를 찾는다. 이론적으로 $k→\infty$일 때 등호가 성립한다. 정책 평가는 $V_{k}=V_{\pi}$을 만족하는 값을 찾기 위해 모든 상태를 돌아다니며 상태 $s$의 가치함수를 갱신한다. 

이렇게 반복적으로 정책 평가를 하기 위해서는 이전 가치함수를 담는 배열과 갱신된 현재 가치함수를 담는 배열이 필요하다. 그렇지만 굳이 이전의 가치함수를 담는 배열을 우리가 저장할 필요가 없으므로, 알고리즘에서는 하나의 배열만을 사용해 배열에 담기는 데이터의 내용을 새로 구한 가치함수의 값으로 갱신한다. 하나의 배열을 쓰면 두 개의 배열을 사용하는 방법보다 더 빠르게 참 가치함수에 수렴한다는 장점이 있다. 

```
loop:
	Δ ← 0 
	for s in S: 
		v ← V(s)
		V(s) ← 벨만 기대 방정식
		Δ ← max(Δ, |v - V(s)|)
loop until Δ < 0
```

- for문을 통해 전체 상태를 훑는다. 이 과정은 가치 함수가 최적이 될 때 중단된다.
- $\Delta$는 이전 가치 함수와 갱신된 가치 함수 사이의 오차를 담는다. 이때, max()를 통해 발생하는 오차 중 최댓값을 업데이트한다. $\Delta$가 0에 수렴한다는 성질을 이용하면 발생하는 최대의 오차가 0에 수렴하기 때문에 현재의 가치 함수가 현 정책에 대한 최적 가치 함수라고 해석할 수 있다.

### 정책 발전

정책 발전은 정책 평가 이후에 이루어지며, 갱신된 가치 함수를 통해 더 좋은 정책을 찾는다. 상태 $s$에서 어떤 행동 $a$를 한 후 정책 $\pi$를 따랐을 때의 행동 가치 함수(큐함수)가 상태 $s$에서 정책 $\pi$를 따랐을 때의 가치 함수보다 크다면, 현재 정책 $\pi$에서 $\pi'$로 정책을 업데이트한다. 

> $q_{\pi}(s,a) ≥ v_{\pi}(s)$   →    $v_{\pi '}(s) ≥ v_{\pi}(s)$
> 

### 정책 이터레이션의 한계

정책 발전이 정책 평가 종료 이후에만 가능하다는 점과 여러 번 많은 양의 계산을 하기 때문에 연산량이 많다는 단점이 있다. 또한 방대한 계산 중 의미 없는 반복 연산을 발생시킬 수 있다.

## 03. 가치 이터레이션

가치 이터레이션은 정책 평가와 정책 발전을 합한 **벨만 최적 방정식**을 사용하는 알고리즘이다. 

> $V_{k+1} = max_a \sum_{a \in A} \pi(a|s) \sum_ {s',r} P(s',r|a,s)[r(s,a) + \gamma V_{k}(s')]$
> 

```
loop:
	Δ ← 0
	for s in S:
		v ← V(s)
		V(s) ← 벨만 최적 방정식
		Δ ← max(Δ, |v - V(s)|)
loop until Δ < 0
```

- 정책 이터레이션과 같은 구조를 같고 있지만 가치 함수가 벨만 최적 방정식이라는 점은 다르다.
- 가치 이터레이션에서 갱신되는 가치 함수는 가능한 행동들의 가치 중 최댓값이다.
- 가치 이터레이션은 **별도의 정책을 갖지 않는다.**

## 04. 비동기 동적 프로그래밍

정책/가치 이터레이션은 전체 상태 집합에 대한 갱신이 이뤄져야 같은 상태에 대해 반복적으로 갱신이 가능하다. 그러나 비동기 동적 프로그래밍은 어떤 상태가 한 번 갱신될 동안 다른 상태를 여러 번 갱신할 수 있다. 이런 비일괄 계산 방법은 좋은 결과를 확실하게 보장하지 못하지만 무의미한 계산을 하지 않아도 되며 학습의 유연성을 보장하고, 상태 간의 효율적인 전파를 돕는다는 점에서 의의가 있다. 

## 05. GPI

GPI(Generalized Policy Iteration)는 정책 평가와 정책 발전이 상호작용하는 구조를 의미한다. 가치 함수는 현재 정책만을 따를 때 안정화(최적 가치 함수)될 수 있고 정책은 계산된 가치 함수에 대해 탐욕적일 때(최적 정책)만 안정화된다. GPI에서 평가와 발전은 경쟁적인 동시에 협력적이다. 따라서 평가와 발전은 최적화라는 공통해를 찾기 위해 움직인다. 


## 코드 실습

1. **에이전트와 환경에 각각 정의되어야 하는 것?**
    - **에이전트(행동, 가치 함수, 정책)**: 어떤 환경을 받을 건지(env=GridWorldEnvironment(~)), value_table, policy_table, gamma, 행동(pi.get_action)
    - **환경(상태, 보상, 모델)**: 시작점, 종료 지점(env.end_point), 그리드월드의 크기(gridworld_size), 가능한 행동의 집합(env.action_space), 행동의 개수(env.num_actions), 전체 상태(env.total_states), render 방법(env.render), 보상 반환(env.get_reward), 다음 상태(env.state_after_action), 그리드월드 경계 내에서만 state가 발생하도록 boundary check
2. **정책 이터레이션 class에 필요한 구성**
    
    ```python
    class PolicyIteration:
    		def __init__(self, env): # 환경, 가능한 행동과 갯수, value_table, policy_table, gamma 정의
    				pass
    		
    		def policy_evaluation(self): # 벨만 기대 방정식을 통해 특정 state의 모든 행동에 대한 가치 함수를 더해 하나의 가치 함수로 value_table 업데이트
    				pass
    		
    		def policy_improvement(self): # 각 행동에 대해 가치 함수를 구한 후, 최대의 가치를 가지는 행동에게만 정책 부여->policy_table 업데이트
    				pass
    				
    		def get_action(self, state): # 특정 state에서 가능한 행동들을 랜덤하게 뽑음
    				pass
    		
    		def get_policy(self, state): # 특정 state에서의 정책을 반환
    				pass
    				
    		def get_value(self, state): # 특정 state에서의 가치 함수를 반환
    				pass		
    ```
    
3. **가치 이터레이션 class에 필요한 구성**
    
    ```python
    class ValueIteration:
    		def __init__(self, env): # env, 가능한 행동과 갯수, value_table, gamma 정의
    				pass
    			
    		def value_iteration(self): # 벨만 최적 방정식을 이용해 특정 state에서 각 행동에 대한 value를 각각 구한 후 최댓값만 value_list에 저장
    				pass
    				
    		def get_action(self): # 특정 state에서 각 행동에 대한 가치 함수를 구한 후 최대의 가치를 가지는 행동만을 저장하고, 그 중에서 랜덤하게 뽑아 반환
    				pass
    				
    		def get_value(self): # 특정 state에서의 가치 함수를 반환
    				pass
    ```
    


## 문제

1. 다이나믹 프로그래밍의 의의를 서술하시오. 
    
    -현재 사용되는 강화학습 방법론은 다이나믹 프로그래밍을 기반으로 발전했다. 
    
2. 정책 평가 종료 이후에만 정책 발전이 이루어지는 것의 단점은?
    
    -정책 평가는 많은 양의 반복 행동을 수행해 연산량이 많다. 이 과정에서 무의미한 연산이 반복될 수도 있고 시간도 오래 걸리는데, 꼭 정책 평가가 끝나야지만 정책 발전이 이루어지는 것은 비효율적이다. 
    
3. 정책 이터레이션에서 초기 정책은 존재하지 않는다.
    
    - x/정책 이터레이션의 정책 평가 과정에서 현재의 정책을 따랐을 때 얻을 수 있는 총보상의 기댓값인 가치 함수를 구하기 때문에 초기 정책은 필요하다. 
    
4. 정책 이터레이션에서 정책 발전이 Q함수 값과 상태 가치 함수의 값을 비교하는 이유는 무엇인가?
    
    -정책 발전 과정에서는 실제로 어떤 정책이 더 좋은지를 선택해야 하기 때문이다. 따라서 현재 상태 $s$에서 어떤 행동 $a$를 한 후 정책 $\pi$를 따랐을 때의 가치함수(Q함수 값)과 상태 가치 함수($v_{\pi}$)의 값을 비교해 더 좋은 정책을 찾아가야 한다. 
    
5. end-point에서의 정책은 무엇인가?
    
    -end-point(종단 상태)에서는 더 이상의 행동이 필요하지 않으며, 가치함수 역시 0으로 주어진다. 이 상태에서는 어떤 행동을 할 확률이 0이기 때문에 end-point에서 정책은 0이 될 것이다. (그러나 굳이 따지자면 end-point에서는 가만히 있는 행동을 하는 것으로 말할 수 있다) 
    
6. 정책 테이블 시각화 이후 분석
    1. (0,0)에서 up, left는 사실 잘못된 정책이 아니다. 근데 왜 값이 0일까?
        
        -(0,0)에서 up, left의 행동을 취하는 것은 그리드월드의 경계를 넘어나는 불가능한 행동이기 때문이다. 또 매우 무의미하다. 또다른 경우를 생각해볼 수 있다. 예를 들어 (2,2)에서 up, left가 가능하지만 이때도 정책의 값은 0이다. 그 이유는 코드의 정책 발전 부분에서 탐욕적인 정책을 하기 때문으로, 정책 이터레이션의 policy_improvement 부분에서 
        
        ```python
         # 받을 보상이 최대인 행동들에 대한 탐욕 정책 발전
                    max_value = np.max(value_list)
                    max_indices = np.where(value_list == max_value)[0]
        
                    # 최댓값이 여러 개일 때 확률을 나누기 위해
                    prob = 1 / len(max_indices)
        
                    for max_idx in max_indices:
                        result[max_idx] = prob
        ```
        
        위와 같은 코드가 짜여진다. value_list에는 [up, down, left, right]에 대한 보상이 각각 저장되어 있지만 그 중에서 최대의 값을 골라 max_value에 저장하고 또 그 행동에 해당하는 인덱스에게만 prob이라는 확률값을 부여하기 때문에, 만약 max_value에 해당하지 않는 인덱스가 있다면 그 인덱스의 값은 여전히 초기값 0으로 남아있다. 이 상황에선 up, left의 인덱스가 그렇다. 
        


## 질문

- 큐러닝이 보완한 살사의 단점과 보완 방법은?
    
    살사는 행동 정책과 목표 정책을 분리되어 있지 않아 초기에 잘못된 학습을 하면 학습이 잘 안 될 가능성이 있다. 큐러닝은 학습의 자료를 저장하는 행동 정책과 실제로 행동하는 목표 정책이 분리되어 있어 살사의 단점을 보완할 수 있다. 
    
- 왜 환경이 지속적으로 변화할 때 스텝사이즈를 평균으로 하는 것보다 일정한 숫자로 고정하는 것이 더 좋을까? (교재 p.122)
    
    스텝 사이즈를 상수로 고정한다는 것은 과거의 반환값보다 현재 에피소드의 반환값을 더 중요하게 고려한다는 의미다. 또한 스텝사이즈가 크다는 것은 과거 반환값의 중요도를 지수적으로 감소시킨다는 것을 의미한다. 만약 상수의 스텝사이즈를 변화시키고 싶다면 한 episode마다 일정한 비율로 감소시키는 코드를 짤 수도 있다. 
    


## 정리

- **RL 정책 평가 1**: DP에서 RL로 넘어가는 가장 기본적인 아이디어는 **몬테카를로 예측**이다. 몬테카를로 예측은 기댓값을 샘플링을 통한 평균으로 대체하는 기법이다. 몬테카를로 예측에서는 에피소드를 하나 진행하고 얻은 반환값 $G_{t}$를 구하고, 이는 하나의 샘플이 되어 한 번의 에피소드를 마친 이후에 모든 상태의 **가치함수를 한 번에 업데이트**한다.
- **RL 정책 평가 2: 시간차 예측**은 타임스텝마다 **큐함수를 업데이트** 한다. 벨만 기대 방정식을 사용해 큐함수를 업데이트 한다.
- **RL 알고리즘 1:  살사**는 GPI의 **정책 평가를 큐함수를 사용한 시간차 예측**으로, **탐욕 정책 발전을 $\varepsilon$-탐욕 정책**으로 변화시킨 강화학습 알고리즘; **온폴리시 시간차 제어; $\varepsilon$**은 **탐험**의 개념을 추가하는 것
    1. $\varepsilon$-탐욕 정책을 통해 샘플 $[S_{t}, A_{t}, R_{t+1}, S_{t+1}, A_{t+1}]$ 획득
    2. 획득한 샘플로 큐함수 $Q(S_{t}, A_{t})$를 업데이트
- **RL 알고리즘 2: 큐러닝은** 행동하는 정책과 학습하는 정책을 따로 분리한다. 에이전트는 실제로 어떤 행동을 했느냐와 상관 없이 현재 상태 $s$의 큐함수를 업데이트할 때에는 다음 상태 $s'$의 최대 큐함수를 이용함→필요한 샘플(s, a, r, s’). **행동 선택은 $\varepsilon$-탐욕 정책**으로, **큐함수 업데이트는 벨만 최적 방정식을 이용**함으로써 탐험vs최적 정책 학습의 딜레마를 극복;**오프폴리시 시간차 제어**
- 살사에서는 큐함수를 업데이트 하기 위해 벨만 기대 방정식을 사용하고 큐러닝에서는 큐함수를 업데이트 하기 위해 벨만 최적 방정식을 사용한다. 


**rendering_method**

1. 먼저 last_point와 traces를 구해줍니다. 
2. 빈 리스트 5개를 만들어 5x5짜리 df를 생성할 수 있도록 해줍니다.
3. render_df를 만들어줍니다.
4. 0으로 채워진 데이터들을 *로 바꿔줍니다
5. 에이전트가 지나간 모든 trace를 X로 바꿔줍니다
6. start_point, end_point, last_point에 각각 S, G, A로 바꿔줍니다.
7. render_df를 출력해줍니다

<aside>
💡 **다른 점 및 구현 방향**



일단 코드가 더 짧다. 기존의 코드는 각 상태에 맞는 X, S, G, A를 부여하고 또 string에 담아야 했지만 위 코드는 이미 df에 구색이 다 맞춰져 있기 때문에 값만 담고 바로 프린트할 수 있다. 또 matplotlib이 그리드 격자를 사용해서 그래프로 그려볼 생각도 했지만…눈금이 잘 안 맞춰짐+조금 더 복잡함 등의 이슈로 그냥 df로 구현해봤다. 또 index/columns를 통해 좌표의 시각화를 지원하고 .index와 .columns를 통해 상태에 대한 접근성이 높아졌다.

</aside>

```python
def render():
		
		last_point=self.traces[-1]
		traces=list(set(self.traces))
		
		lists=[[0,0,0,0,0] for i in range(self.width)]
		render_df={i:lists[i] for i in range(self.width)}
		
		for trace in traces:
				render_df.loc[trace[0], trace[1]]="X" #loc: label-based/ilon: index-based
		render_df.loc[self.start_point[0], self.start_point[1]]="S"
		render_df.loc[self.end_point[0], self.end_point[1]]="G"
		render_df.loc[last_point[0], last_point[1]]="A"
		
		print(render_df)
```
