# 4주차_강화학습

태그: 완료

## 아이디어

기존의 몬테카를로, 살사, 큐러닝 등의 테이블 기반의 강화학습은 모든 상태를 명확히 알고 있고 그 개수가 적어야 적용 가능했다. 그러나 딥러닝 기반 강화학습은 상태-행동 쌍에 대한 가치와 정책에 딥러닝 신경망을 통해 근사하여 구한다. 이때 가치 함수를 신경망으로 근사하는 방법을 가치 그래디언트, 정책을 신경망으로 근사하는 방법을 정책 그래디언트라고 한다. 

## 가치 그래디언트

가치 함수를 신경망을 통해 근사한다. 이때 신경망의 input은 상태이고 output은 큐함수다. 대표적인 가치 그래디언트에는 SARSA의 가치 함수를 신경망으로 대체하는 Deep SARSA가 있다. 

### Deep SARSA

딥살사에서는 정답 역할의 $R_{t+1}+\gamma Q(S_{t+1},A_{t+1})$와 예측값인 $Q(S_{t}, A_{t})$의 오차를 줄이며 학습한다. 이때 오차를 줄이는 방법으로 경사하강법이 사용되며 오차를 계산하기 위해 MSE(Mean Squared Estimation)이 사용된다. 이때 신경망 노드의 개수는 #action이다. 

$$
Q_{\theta}(S_t,A_t) ← Q_{\theta}(S_t,A_t)  + \alpha (𝑅_{𝑡+1} + 𝛾𝑄_{\theta}(𝑠_{t+1},𝑎_{t+1}) − 𝑄_{\theta}(𝑠_t,𝑎_t))^2
$$

$\theta$는 신경망의 파라미터로, 오차를 줄이는 방향으로 업데이트해야 한다. $\theta$에 따라 가치 함수가 결정된다. 

## 정책 그래디언트

정책을 신경망을 통해 근사한다. 이때 신경망의 input은 상태이고 output은 어떤 상태에서 어떤 행동을 할 확률인 정책이다. 또한 가치 함수를 기반으로 탐욕적으로 정책을 선택하는 일반적인 정책과는 달리, 정책 그래디언트는 가치를 거치지 않고 신경망으로 정책을 근사하기 때문에 가치 함수를 구할 필요가 없다. 

### 정책 $\pi_{\theta}(a|s)$

$\theta$는 신경망의 파라미터로, 이에 따라 정책이 결정된다. 

### 학습 목표

정책 그래디언트는 최적 정책을 찾고자 한다. 최적 정책은 최종 누적 보상을 최대로 만드는 상태-행동 쌍이 정의된 정책이다. 따라서 정책 그래디언트의 신경망은 상태를 input으로 받았을 때, 누적 보상을 최대로 만드는 행동을 내놓는 것을 목표로 한다.  

- 학습 목표: $maxJ(\theta)$
- 누적 보상: $J(\theta)$=$v_{\pi_{\theta}}(s_{0})$=처음 상태$(s_{0})$에서 정책 $v_{\pi_{\theta}}$을 따라갔을 때 얻는 가치
- 정책 그래디언트 업데이트

![Untitled](https://github.com/user-attachments/assets/2d1be7e6-066f-45b9-aff1-d2cfe449b4af)

$$
∇_{\theta}J(\theta) = E_{\pi_{\theta}}[∇_{\theta}log(\pi_{\theta}(a|s))*q_{\pi}(s,a)]
$$

$*J(\theta)$를 $\theta$에 대해 미분한 것으로, 가치에 대한 기댓값이다.* 

---

💡가치 그래디언트는 정답과 예측 사이의 오차를 최소화하는 $\theta$를 찾기 때문에 경사하강법을 사용하지만, 정책 그래디언트는 누적 보상을 최대화하는 $\theta$를 찾기 때문에 경사상승법을 이용한다. 

## 문제

1. 몬테카를로/살사/큐러닝으로 해결할 수 없는 문제사항은 무엇인가?
    
    -이 방법론들은 테이블 기반이기 때문에 만약 상태를 명확히 알 수 없거나 상태의 개수가 무한히 많다면 동작하지 않는다. 
    
2. 딥살사의 nn 입출력은 무엇인가?
    
    -입력은 상태, 출력은 큐함수다. 
    
3. 딥살사에서 업데이트해야 하는 파라미터는 무엇인가?
    
    -파라미터는 $\theta$로, 큐함수를 근사하는 신경망의 파라미터이다. 딥살사에서는 경사하강법을 통해 MSE로 계산된 오차를 최소화하는 $\theta$를 찾는다. 
    
4. 강화학습에서 탐색의 중요성
    
    -현실에선 상태가 한정적이지 않고 무궁무진하기 때문에 최대한 다양한 상황에 대한 탐색과 학습을 하는 것이 중요하다. 그래서 $\varepsilon-$탐욕 정책과 같이 다양한 상태를 탐험할 수 있는 정책을 시행한다. 
    
5. 딥살사의 nn 정답값과 예측값은 각각 무엇인가?
    
    -정닶값은 $R_{t+1}+\gamma Q(S_{t+1}, A_{t+1})$이고 예측값은 $Q(S_{t}, A_{t})$이다. 
    
6. epsilon값이 고정되는 것과 줄어드는 것은 어떤 효과를 지니는가?
    
    -초반에는 랜덤하게 행동하며 다양한 상황에 대해 학습하지만 epsilon 값이 줄어들면 점점 예측한 대로 에이전트가 행동한다. 
    
7. 정책 그래디언트는 경사하강법을 이용한다.
    
    -x/ 가치 그래디언트는 정답과 예측의 오차를 최소화하는 $\theta$를 찾기 위해 경사하강법을 사용하지만 정책 그래디언트는 누적 보상을 최대로 하는 $\theta$를 찾기 때문에 경사상승법을 이용한다. 
    
8. REINFORCEMENT 알고리즘의 업데이트 기준과 업데이트 시점은 무엇인가?
    
    -업데이트 기준은 반환값 $G_{t}$이며 업데이트 시점은 한 에피소드가 끝난 이후이다. 
    
9. 가치 그래디언트와 정책 그래디언트의 비교
    1. nn의 출력값은 각각 무엇을 의미하는가?
        1. 가치 그래디언트 nn의 출력은 큐함수로, 어떤 상태에서 어떤 행동을 했을 때 얻을 수 있는 가치이다. 정책 그래디언트 nn의 출력은 정책으로, 어떤 상태에서 어떤 행동을 할 확률이다. 따라서 가치 함수를 따로 구하지 않아도 된다. 
    2. 가치 그래디언트와 정책 그래디언트는 각각 어떤 활성화 함수를 사용하는가?
        1. 가치 그래디언트의 출력은 큐함수로, 선형 활성화 함수를 사용한다. 반면 정책 그래디언트의 출력은 정책, 즉 확률이기 때문에 0에서 1사이의 값이 나온다. 따라서 softmax 활성화 함수를 이용한다. 

## 질문

- 카트폴에서 에이전트가 해야 할 일을 하기 위해 주어져야 하는 정보는 무엇인가? (힌트: 4개)
    
    [수평선 상의 카트 위치 x, 수평선 상의 카트 속도 x’, 폴의 수직선으로부터 기운 각도 $\theta$, 폴의 각속도 $\theta$ ‘]
    
- 큐러닝과 인공신경망을 함께 사용하려면 무엇이 필요한가?
    
    경험 리플레이가 필요하다. 경험 리플레이란 에이전트가 환경에서 탐험하며 얻은 샘플 (s,a,r,s’)를 리플레이 메모리에 저장하는 것이다. 이후 리플레이 메모리에서 무작위로 학습 데이터를 추출하고 학습하는 과정을 반복한다. 이때 메모리에는 용량이 제한되어 있기에 꽉 차면 초반에 있던 메모리를 삭제하는 과정을 통해 나중에는 더 좋은 샘플들로 메모리가 채워지게 된다. 
    
- 리플레이 메모리를 사용하면 딥살사의 어떤 한계를 극복할 수 있는가?
    
    딥살사는 온폴리시 알고리즘으로, 한 번 안 좋은 상황에 빠지면 그 상황에 맞게 학습이 진행되어 학습이 잘 이루어지지 않을 수 있다. 그러나 리플레이 메모리는 학습 데이터(샘플)을 무작위로 추출하기 때문에 샘플들 사이에 시간적 상관관계가 없으므로 이런 일이 발생하지 않는다. 
    
- DQN의 타깃 신경망은 어떤 역할을 하는가?
    
    DQN은 기본적으로 추정값을 추정하는 값을 구하는 부트스트랩을 이용한다. 그러나 부트스트랩의 단점은 정답이 되는 추정값이 변하고, 추정값을 구하는 인공신경망도 계속 변한다는 것이다. 따라서 DQN의 타깃 신경망은 정답을 만드는 신경망으로서, 인공신경망에서는 정답에 해당하는 값을 구하도록 학습한다. 이후 에피소드마다 이 인공신경망이 타깃 신경망으로 업데이트된다. 이로써 부트스트랩의 문제가 심화되는 것을 방지할 수 있다.
